Spectral simplicity of apparent complexity. II. Exact complexities and complexity spectra
The meromorphic functional calculus developed in Part I overcomes the nondiagonalizability of linear operators that arises often in the temporal evolution of complex systems and is generic to the metadynamics of predicting their behavior. Using the resulting spectral decomposition, we derive closed-form expressions for correlation functions, finite-length Shannon entropy-rate approximates, asymptotic entropy rate, excess entropy, transient information, transient and asymptotic state uncertainties, and synchronization information of stochastic processes generated by finite-state hidden Markov models. This introduces analytical tractability to investigating information processing in discrete-event stochastic processes, symbolic dynamics, and chaotic dynamical systems. Comparisons reveal mathematical similarities between complexity measures originally thought to capture distinct informational and computational properties. We also introduce a new kind of spectral analysis via coronal spectrograms and the frequency-dependent spectra of past-future mutual information. We analyze a number of examples to illustrate the methods, emphasizing processes with multivariate dependencies beyond pairwise correlation. This includes spectral decomposition calculations for one representative example in full detail.
I. INTRODUCTION
Tracking the evolution of a complex system, a time series of observations often appears quite complicated in the sense of temporal patterns, stochasticity, and behavior that require significant resources to predict. Such complexity arises from many sources. Apparent complexity, even in simple systems, can be induced by practical measurement and analysis issues, such as a small sample size, inadequate collection of probes, noisy or systematically distorted measurements, coarse-graining, out-of-class modeling, nonconvergent inference algorithms, and so on. The effects can either increase or decrease apparent complexity, as they add or discard information, hiding the system of interest from an observer to one degree or another. Assuming perfect observation, complexity can also be inherent in nonlinear stochastic dynamical processes—deterministic chaos, superexponential transients, high state-space dimension, nonergodicity, nonstationarity, and the like. Even in ideal settings, the smallest sufficient set of a system's maximally predictive features is generically uncountable, making approximations unavoidable, in principle [1]. With nothing else said, these facts obviate physical science's most basic goal—prediction—and, without that, they preclude understanding how nature works. How can we make progress?
The prequel, Part I [18], argued that this is too pessimistic a view. It introduced constructive results that address the hidden structure and the challenges associated with predicting complex systems. It showed that questions regarding correlation, predictability, and prediction each require their own analytical structures. Measures of correlation—including autocorrelation functions, power spectra, and Greeen–Kubo transport coefficients—are direct signatures of the transition dynamic of any hidden Markov model (HMM) representation of a process. However, Part I [18] explained that synchronizing to the hidden linear dynamic in systems induces a nondiagonalizable metadynamics, even if the dynamics are diagonalizable in their underlying state-space. Hence, questions about predictability—for example: “How much of the future can be predicted from past observations?” and “What is the irreducible randomness of the process?”—and the burdens of actually predicting—“How much memory must be allocated to predict what is predictable?” and “How much time must be invested before an observer is sufficiently synchronized to make good predictions?”—are instead answered by the nondiagonalizable transition dynamics of the so-called mixed state presentations (MSPs) of the process.
Part I [18] gave operator expressions that begin to answer these questions. However, assuming normal and diagonalizable dynamics, so familiar in mathematical physics, simply fails in this setting. Thus, nondiagonalizable dynamics presented an analytical roadblock. Part I [18] reviewed a calculus for functions of nondiagonalizable operators—the recently developed meromorphic functional calculus of Ref [2]—that directly addresses nondiagonalizability, giving constructive calculational methods and algorithms. Part I [18] also highlighted a spectral weighted directed-graph theory that can give useful shortcuts for determining a process' spectral decomposition.
Part II now goes a step further, providing the first closed-form expressions for many of the complexity measures in wide use. In the following through the derivations, we discover a timescale—the symmetry collapse index—that indicates the sophistication of finite computational structures in infinite-Markov-order processes. We are also led to the introduction of complexity spectra that give a frequency-decomposition of predictable features.
In the following, we consider wide-sense stationary stochastic processes …X_{t−1}X_{t}X_{t+1}… with a discrete domain. The random variable X_{t} can take on values x∈A, which we assume to be a finite set. The interdependencies among the random variables can be interpreted as intrinsic computations carried out by the process under observation. While such stochastic processes are natural in discrete-time signal processing [3,4] and on discrete lattices [5], they are sometimes also obtained from continuous-time chaotic systems via regular sampling. Moreover, while finite alphabets A are intrinsic to certain sources, they may also be obtained from continuous phase-spaces, either by measurement precision limitations or by intentional parsimony as in the case of generating partitions used in symbolic dynamics [6–8]. Such processes are typically non-Markovian and can appear very complicated, but we unravel their complexities via analysis of a HMM representation of the process—where the HMM may be obtained from first principles or may be inferred by any number of methods (see Ref [9] and references therein).
A. Notational review
Part II here uses Part I's [18] notation and assumes familiarity with its results.
In Part I [18], we reviewed relevant background in stochastic processes and their complexities and the hidden Markov models (HMMs) that generate them. Part I [18] delineated several classes of HMMs—Markov chains, unifilar HMMs, and nonunifilar HMMs. It also reviewed their mixed-state presentations (MSPs)—HMM generators of a process that track distributions induced by observation. Related constructions included the mixed-functional presentations and cryptic-operator presentations. MSPs are keys to calculating complexity measures within an information-theoretic framing. Part I [18] then showed how each complexity measure reduces to a linear algebra of an appropriate HMM adapted to the cascading- or accumulating-question genre. It summarized the meromorphic functional calculus and several of its mathematical implications in relation to spectral projection operators.
Recall that a distribution η over the states S of an HMM M=(S,A,{T^{(x)}}{x∈A},S_{0}∼η) induces a probability distribution over subsequent words x_{0:L}=x_{0}x_{1}…x_{L−1} generated by the process
where the labeled transition matrices, with elements Ti,j(x)=Pr(X_{t}=x,S_{t+1}=s_{j}|S_{t}=s_{i}), sum to the row-stochastic transition matrix T=∑x∈AT^{(x)} and |1〉 is the all-ones vector such that T|1〉=|1〉.
Ergodic processes will have a single stationary distribution π, such that 〈π|T=〈π|. (Nonergodic processes can still have a specified stationary distribution π over states, but it could no longer be uniquely identified from T alone.) The synchronizing MSP (S-MSP) introduced in Part I [18] is a metadynamic of how distributions over S are updated by observation, starting in the mixed state π and thus starting in the peaked distribution δ_{π} over the set of mixed states. The full set of observation-induced mixed states is R_{π}=∪_{w∈L}{〈π|T^{(w)}〈π|T^{(w)}|1〉}, where L⊂A^{*} is the language of observable words. Although its dynamic is “meta” in relation to the dynamic of the original HMM, the MSP itself is also a HMM with labeled transition operators W^{(x)} summing to the row-stochastic and generically nondiagonalizable mixed-state-to-state transition dynamic W.
Recall that the meromorphic functional calculus prescribes the construction of a new operator from an arbitrary function f of a linear operator A as
where Λ_{A} is the set of A's eigenvalues, ν_{λ} is the index of the eigenvalue λ (i.e., the size of the largest Jordan block associated with λ), z∈ℂ, C_{λ} is a positively oriented Jordan curve around λ that includes no other singularities besides possibly at λ itself, and A_{λ,m}=A_{λ}(λI−A){m} where I is the identity operator and A_{λ} is the spectral projection operator, more simply known as the eigenprojector. The eigenprojector can be constructed as
where |λk(m+1)〉 and 〈λk(m+1)| are the mth generalized right and left eigenvectors, respectively, of the kth Jordan chain such that
and
for 0≤m≤m_{k}−1, where |λj(0)〉=0→ and 〈λj(0)|=0→. Specifically, |λk(1)〉 and 〈λk(1)| are conventional right and left eigenvectors, respectively—these are sometimes written more simply as |λ〉 and 〈λ|, when the algebraic multiplicity a_{λ}=1 and there is thus no risk of ambiguity. After imposing normalization, we find that
Perhaps counterintuitively, this implies for example that the most generalized right eigenvectors are dual to the least generalized left eigenvectors.
Usefully, Part I [18] also demonstrated how eigenvalues can be read off from graph motifs and how eigenvectors and spectral projection operators can be built up hierarchically. Part I [18] should be consulted for further details.
B. Outline of main results
With Part I's [18] toolset laid out, Part II now derives the promised closed-form complexities of a process. Section II investigates the range of possible behaviors for correlation and myopic uncertainty via convergence to asymptotic correlation and asymptotic entropy rates. Section III then considers measures related to accumulating quantities during the transient relaxation to synchronization. Section IV introduces closed-form expressions for a wide range of complexity measures in terms of the spectral decomposition of a process' dynamic. It also introduces complexity spectra and highlights common simplifications for special cases, such as almost diagonalizable dynamics. The excess entropy spectrum should be an especially useful diagnostic tool since it shows not only the extent but also the timescales over which the past can yield predictions about the future. Additionally, we discover a new timescale—the symmetry collapse index—reflective of computational structures in infinite-Markov-order processes. Section V gives a new kind of signal analysis in terms of coronal spectrograms. A suite of examples in Secs. VI and VII ground the theoretical developments and are complemented with an in-depth pedagogical example worked out in Appendix A. Finally, we conclude with a brief retrospective of Parts I and II and turn an eye towards future applications.
II. CORRELATION AND MYOPIC UNCERTAINTY
Using Part I's [18] methods, our first step is to solve for the autocorrelation function
and the myopic uncertainty or finite-history Shannon entropy rate
While the first of these indicates how a time-series correlates with itself across time, the second indicates how much new information is generated by a single observation beyond what could be inferred from the last L observations. A comparison is informative. We then determine the asymptotic correlation and myopic uncertainty from the resulting finite-L expressions.
A. Nonasymptotics
A central result in Part I [18] was the spectral decomposition of powers of a linear operator A, even if that operator is nondiagonalizable. Recall that for any L∈ℂ
where (Lm) is the generalized binomial coefficient
(L0)=1, and [0∈Λ_{A}] is the Iverson bracket. The latter takes on value 1 if zero is an eigenvalue of A and 0 if not.
In light of this, the autocorrelation function γ(L) is simply a superposition of weighted eigen-contributions. Part I [18] showed that Eq. (1) has the operator expression
where T is the transition dynamic of any HMM presentation of the process, A is the output symbol alphabet, and we defined the row vector
and the column vector
Substituting Part I's [18] spectral decomposition of matrix powers, Eq. (3), directly leads to the spectral decomposition of γ(L) for nonzero integer L
We denote the persistent first term of Eq. (5) as γ_{⇝}, and note that it can be expressed as
where T^{D} is T's Drazin inverse. We denote the ephemeral second term as γ_{⊸}, which can be written as
where T_{0} is the eigenprojector associated with the eigenvalue of zero; T_{0}=0 if 0∉Λ_{T}.
From Eq. (5), it is now apparent that the index of T's zero eigenvalue gives a finite-horizon contribution (γ_{⊸}) to the autocorrelation function. Beyond index ν_{0} of T, the only L-dependence comes via a weighted sum of terms of the form (|L|−1m)λ^{|L|−1−m}—polynomials in L times decaying exponentials. The set {〈πA¯|T_{λ,m}|A1〉} simply weights the amplitudes of these contributions. In the familiar diagonalizable case, the behavior of autocorrelation is simply a sum of decaying exponentials λ^{|L|}.
Similarly, in light of Part I's [18] expression for the myopic entropy rate in terms of the MSP—starting in the initial unsynchronized mixed-state π and evolving the state of uncertainty via the observation-induced MSP transition dynamic W
where
is simply the column vector whose ith entry is the entropy of transitioning from the ith state of S-MSP—and its spectral decomposition of A^{L}, we find the most general spectral decomposition of the myopic entropy rates h_{μ}(L) to be
We denote the persistent first term of Eq. (8) as h_{⇝}, and note that it can be expressed directly as
where W^{D} is the Drazin inverse of the mixed-state-to-state net transition dynamic W. We denote the ephemeral second term as h_{⊸}, which can be written as
From Eq. (8), we see that the index of W's zero eigenvalue gives a finite horizon contribution (h_{⊸}) to the myopic entropy rate. Beyond index ν_{0} of W, the only L-dependence comes via a weighted sum of terms of the form (L−1m)λ^{L−1−m}—polynomials in L times decaying exponentials. The set {〈δ_{π}|W_{λ,m}|H(W^{A})〉} weighs the amplitudes of these contributions.
For stationary processes we anticipate that for all ζ∈{λ∈Λ_{W}:|λ|=1,λ≠1},〈δ_{π}|W_{ζ}=0 and thus 〈δ_{π}|W_{ζ}|H(W^{A})〉=0. Hence, we can save ourselves from superfluous calculation by excluding the nonunity eigenvalues on the unit circle, when calculating the myopic entropy rate for stationary processes. In the diagonalizable case, again, its behavior is simply a sum of decaying exponentials λ^{L}.
In practice, γ_{⊸} often vanishes, whereas h_{⊸} is often nonzero. This practical difference between γ_{⊸} and h_{⊸} stems from the difference between typical graph structures of the respective dynamics. For a stationary process' generic transition dynamic, zero eigenvalues (and so ν_{0}(T) of T) typically arise from hidden symmetries in the dynamic. In contrast, the MSP of a generic transition dynamic often has tree-like ephemeral structures that are primarily responsible for the zero eigenvalues (and ν_{0}(W)). Nevertheless, despite their practical typical differences, the same mathematical structures appear and contribute to the most general behavior of each of these cascading quantities.
The breadth of qualitative behaviors shared by autocorrelation and the myopic entropy rate is common to the solution of all questions that can be reformulated as a cascading hidden linear dynamic; the myopic state uncertainty H^{+}(L) is just one of many other examples. As we have already seen, however, different measures of a process reflect signatures of different linear operators.
Next, we explore similarities in the qualitative behavior of asymptotics and discuss the implications for correlation and the entropy rate.
B. Asymptotic correlation
The spectral decomposition reveals that the autocorrelation converges to a constant value as L→∞, unless T has eigenvalues on the unit circle besides unity itself. This holds if index ν_{0} is finite, which it is for all processes generated by finite-state HMMs and also many infinite-state HMMs. If unity is the sole eigenvalue with magnitude one, then all other eigenvalues have magnitude less than unity and their contributions vanish for large enough L. Explicitly, if ν_{0}(T)<∞ and argmax_{λ∈ΛT}|λ|={1}, then
This used the fact that ν_{1}=1 and that T_{1}=|1〉〈π| for an ergodic process. This confirms the expectation that 〈XtX¯t+L〉{t}−〈Xt〉{t}〈X¯t〉{t}→0 as L→∞ if argmax_{λ∈ΛT}|λ|={1}, since the random variables (X_{t} and X¯{t+L}) then become de-correlated in the limit of large L.
If other eigenvalues in Λ_{T} besides unity lie on the unit circle, then the autocorrelation approaches a periodic sequence as L gets large. This latter case includes not only deterministically periodic processes, but also stochastic processes with periodic randomness.
C. Asymptotic entropy rate
By the Perron–Frobenius theorem, ν_{λ}=1 for all eigenvalues of W on the unit circle. Hence, in the limit of L→∞, we obtain the asymptotic entropy rate for any stationary process
since, for stationary processes, 〈δ_{π}|W_{ζ}=0 for all ζ∈{λ∈Λ_{W}:|λ|=1,λ≠1}. For nonstationary processes, the limit may not exist, but h_{μ} may still be found in a suitable sense as a function of time. If the process has only one stationary distribution over mixed states, then W_{1}=|1〉〈π_{W}| and we have
where π_{W} is the stationary distribution over W's states, found either from 〈π_{W}|=〈δ_{π}|W_{1} or from solving 〈π_{W}|W=〈π_{W}|.
A simple but interesting example of when ergodicity does not hold is the multi-armed bandit problem [10,11]. In this, a realization is drawn from an ensemble of differently biased coins or, for that matter, over any other collection of IID processes. More generally, there can be many distinct memoryful stationary components from which a given realization is sampled, according to some probability distribution. With many attracting components we have the stationary mixed-state eigenprojector W_{1}=∑k=1a_{1}|1_{k}〉〈1_{k}|, with 〈1_{j}|1_{k}〉=δ_{j,k}, where the algebraic multiplicity a_{1}(T)=a_{1}(W) of the “1” eigenvalue is the number of attracting components. The entropy rate becomes
Above, 〈δ_{π}|1_{k}〉 is the probability of ending up in component k, while 〈1_{k}|H(W^{A})〉 is component k's entropy rate. Thus, if nonergodic, the process' entropy rate may not be the same as the entropy of any particular realization. Rather, the process' entropy rate is a weighted average of those for the ensemble of sequences constituting the process.
For unifilar M, the topology, transition probabilities, and stationary distribution over the recurrent states are the same for both M and its S-MSP. Hence, for unifilar M we have
One can easily show that Eq. (16) is equivalent to the well-known closed-form expression for h_{μ} for unifilar presentations
For nonunifilar presentations, however, we must use the more general result of Eq. (13). This is similar to the calculation in Eq. (17), but must be performed over the recurrent states of a mixed-state presentation, which may be countable or uncountable.
III. ACCUMULATED TRANSIENTS FOR DIAGONALIZABLE DYNAMICS
In the diagonalizable case, autocorrelation, myopic entropy rate, and myopic state uncertainty reduce to a sum of decaying exponentials. Correspondingly, we can find the power spectrum, excess entropy, and synchronization information, respectively, via geometric progressions.
For example, if W is diagonalizable and has no zero eigenvalue, then the myopic entropy rate reduces to
where 〈δ_{π}|W_{1}|H(W^{A})〉 is identifiable as the entropy rate h_{μ}.
It then follows that the excess entropy, which is the mutual information E=I[X←;X→] between the past and the future—and thus how much of the future can be predicted from the past—is
Note that larger eigenvalues (closer to unity magnitude) drive the denominator 1−λ closer to zero and, thus, increase 11−λ. Hence, larger eigenvalues—controlling modes of the mixed-state transition matrix that decay slowly—have the potential to contribute most to excess entropy. Small eigenvalues—quickly decaying modes—do not contribute significantly. Putting aside the language of eigenvalues, one can paraphrase: slowly decaying transient behavior (of the distribution of distributions over process states) has the most potential to make a process appear complex.
Continuing, the transient information, used in the context of synchronization and distinguishing periodic structures [12], is
We now see that the transient information is very closely related to the excess entropy, differing only via the square in the denominators. This comparison between E and T closed-form expressions suggests an entire hierarchy of informational quantities based on eigenvalue weighting.
Performing a similar procedure for the synchronization informationS′ [13] shows that
where |H[η]〉≡∑η∈R_{π}|δ_{η}〉H[η] is the column vector of entropies associated with each mixed-state.
The expressions reveal a remarkably close relationship between S′ and E. Define 〈·|≡∑L=0∞〈δ_{π}|W^{L}. Then
The relationship is now made plain
Although a bit more cumbersome, perhaps better intuition emerges if we rewrite 〈·| as 〈∫Pr(η,L)dL|.
Again, large eigenvalues—slowly decaying modes of the mixed-state transition matrix—can make the largest contribution to synchronization information; small eigenvalues correspond to quickly decaying modes that do not have the opportunity to contribute. In fact, the potential of large eigenvalues to make large contributions is a recurring theme for many questions one has about a process. Simply stated, long-term behavior—what we often interpret as “complex” behavior—is dominated by a process's largest-eigenvalue modes.
That said, a word of warning is in order. Although large-eigenvalue modes have the most potential to make contributions to a process's complexity, the actual set of largest contributors also depends strongly on the amplitudes {〈δ_{π}|W_{λ}|…〉}, where |…〉 is some quantifier vector of interest; e.g., |…〉=|H[η]〉,|…〉=|H(W^{A})〉, or |…〉=|1〉.
Hence, there is as-yet unanticipated similarity between E and T and another between E and S′—at least assuming diagonalizability. We would like to know the relationships between these quantities more generally. However, deriving the general closed-form expressions for accumulated transients is not tractable via the current approach. Rather, to derive the general results, we deploy the meromorphic functional calculus directly at an elevated level, as we now demonstrate.
IV. EXACT COMPLEXITIES AND COMPLEXITY SPECTRA
We now derive the most general closed-form solutions for several complexity measures, from which expressions for related measures follow straightforwardly. This includes an expression for the past–future mutual information or excess entropy, identifying two distinct persistent and transient components, and a novel extension of excess entropy to temporal frequency spectra components. We also give expressions for the synchronization information and power spectra. We explicitly address the class—a common one we argue—of almost diagonalizable dynamics. The section finishes by highlighting finite-order Markov order processes that, rather than being simpler than infinite Markov order processes, introduce technical complications that must be addressed.
Before carrying this out, we define several useful objects. Let ρ(A) be the spectral radius of matrix A
For stochastic W, since ρ(W)=1, let Λ_{ρ(W)} denote the set of eigenvalues with unity magnitude
We also define
and
Eigenvalues with unity magnitude that are not themselves unity correspond to perfectly periodic cycles of the state-transition dynamic. By their very nature, such cycles are restricted to the recurrent states. Moreover, we expect the projection operators associated with these cycles to have no net overlap with the start-state of the MSP. So, we expect
for all λ∈Λ_{ρ(W)}∖{1}. Hence
We will also use the fact that, since ρ(Q)<1
and furthermore
as a consequence of Eq. (21) and our spectral decomposition.
Having seen complexity measures associated with prediction all take on a similar form in terms of the S-MSP state-transition matrix, we expect to encounter similar forms for generically nondiagonalizable state-transition dynamics.
A. Excess entropy
We are now ready to develop the excess entropy in full generality. Our tools turn this into a direct calculation. We find
Note that (I−Q){−1}=inv(I−Q) here, since unity is not an eigenvalue of Q. Indeed, the unity eigenvalue was explicitly extracted from the former matrix to make an invertible expression.
For an ergodic process, where W_{1}=|1〉〈π_{W}|, this becomes
Computationally, Eq. (23) is wonderfully useful. However, the subtraction of h_{μ} is at first mysterious. Especially so, when compared to the compact result for the excess-entropy spectral decomposition in the diagonalizable case given by Eq. (18).
Let us explore this. Recall that Ref [2] showed
for any stochastic matrix T, where T_{1} is the projection operator associated with eigenvalue λ = 1. From this, we see that the general solution for E takes on its most elegant form in terms of the Drazin inverse of I – W
Recall too Part I's [18] explicit spectral decomposition
which uses the companion operatorsT_{λ,m} from there. From this and Eq. (25), we see that the past–future mutual information—the amount of the future that is predictable from the past—has the general spectral decomposition
B. Persistent excess
In light of Eq. (9), we see that there are two qualitatively distinct contributions to the excess entropy E=E_{⇝}+E_{⊸}. One comprises the persistent leaky contributions from all L
and the other is a completely ephemeral piece that contributes only up to W's zero-eigenvalue index ν_{0}
C. Excess entropy spectrum
Equation (25) immediately suggests that we generalize the excess entropy, a scalar complexity measure, to a complexity function with continuous part defined in terms of the resolvent—say, via introducing the complex variable z
Such a function not only monitors how much of the future is predictable, but also reveals the time scales of interdependence between the predictable features within the observations. Directly taking the z-transform of h_{μ}(L) comes to mind, but this requires tracking both real and imaginary parts or, alternatively, both the magnitude and complex phase. To ameliorate this, we employ a transform of a closely related function that contains the same information.
Before doing so, we should briefly note that ambiguity surrounds the appropriate excess-entropy generalization. There are many alternate measures that approach the excess entropy as frequency goes to zero. For example, directly calculating from the meromorphic functional calculus, letting z=e^{iω} we find
We are challenged, however, to interpret the fact that Re〈δ_{π}|(eiωI−W){−1}|H(W^{A})〉+12h_{μ} is not necessarily positive at all frequencies. Another direct calculation shows that
Enticingly, Re〈δ_{π}|e^{iω}(eiωI−W){−1}|H(W^{A})〉−12h_{μ} appears to be positive over all frequencies for all examples checked. It is not immediately clear which, if either, is the appropriate generalization, though. Fortunately, the Fourier transform of a two-sided myopic-entropy convergence function makes our upcoming definition of E(ω) interpretable and of interest in its own right.
Let  be the two-sided myopic entropy convergence function defined by
For stationary processes, it is easy to show that H[X_{0}|X_{−L+1:0}]=H[X_{0}|X_{1:L}], with the result that  is a symmetric function. Moreover, h then simplifies to
where h_{μ}(0)≡log_{2}|A| and, as before, h_{μ}(L)=H[X_{L}|X_{1:L}] for L≥1 with h_{μ}(1)=H[X_{1}].
The symmetry of the two-sided myopic entropy convergence function h guarantees that its Fourier transform is also real and symmetric. Explicitly, the continuous part of the Fourier transform turns out to be
a strictly real and symmetric function of the angular frequency ω. Here, R is the redundancy of the alphabet R≡log_{2}|A|−h_{μ}, as in Ref [12].
The transform  also has a discrete impulsive component. For stationary processes, this consists solely of the Dirac δ-function at zero frequency
Recall that the Fourier transform of a discrete-domain function is 2π-periodic in the angular frequency ω. This δ-function is associated with the nonzero offset of the entropy convergence curve of positive-entropy-rate processes. The full transform is
Direct calculation using Ref [2]'s meromorphic functional calculus shows that
This motivates introducing the excess-entropy spectrumE(ω)
The excess-entropy spectrum rather directly displays important frequencies of apparent entropy reduction. For example, leaky period-5 processes have a period-5 signature in the excess entropy spectrum.
As with its predecessors, the excess-entropy spectrum also has a natural decomposition into two qualitatively distinct components
The excess-entropy spectrum gives an intuitive and concise summary of the complexities associated with a process' predictability. For example, given a graph of the excess entropy spectrum, the past–future mutual information can be read off as the height of the continuous part of the function as it approaches zero frequency
Indeed, the limit of zero frequency is necessary due to the δ-function in the Fourier transform at exactly zero frequency
Reflecting on this, the δ-function indicates one of the reasons the excess entropy has been difficult to compute in the past. This also sheds light on the role of the Drazin inverse: It removes the infinite asymptotic accumulation, revealing the transient structure of entropy convergence.
We also have a spectral decomposition of the excess-entropy spectrum
where, in the last equality, we assume that W_{0} is real. This shows that, in addition to the contribution of typical leaky modes of decay in entropy convergence, the zero-eigenvalue modes contribute uniquely to the excess entropy spectrum. In addition to Lorentzian-like spectral curves contributed by leaky periodicities in the MSP, the excess-entropy spectrum also contains sums of cosines up to a frequency controlled by index ν_{0}, which corresponds to the depth of the MSP's nondiagonalizability. This is simply the duration of ephemeral synchronization in the time domain.
D. Synchronization information
Once expressed in terms of the S-MSP transition dynamic, the derivation of the excess synchronization information S′ closely parallels that of the excess entropy, only with a different ket |·〉 appended. We calculate, as before, finding
For an ergodic process where W_{1}=|1〉〈π_{W}|, this becomes
From Eq. (24), we see that the general solution for S′ takes on its most elegant form in terms of the Drazin inverse of I – W
From Eqs. (32) and (26), we also see that the excess synchronization information has the general spectral decomposition
Again the form of Eq. (32) suggests generalizing synchronization information from a complexity measure to a complexity function S(ω). In this case, the result is simply related to the Fourier transform of the two-sided myopic state-uncertainty H(L).
E. Power spectra
The extended complexity functions, E(ω) and S(ω) just introduced, give the same intuitive understanding for entropy reduction and synchronization, respectively, as the power spectrum P(ω) gives for pairwise correlation. Recall from Part I [18] that the power spectrum can be written as
We see that (eiωI−T){−1} is the resolvent of T evaluated along the unit circle z=e^{iω} for ω∈[0,2π). Hence, by Part I's [18] decomposition of the resolvent, the general spectral decomposition of the continuous part of the power spectrum is
As with E(ω) and S(ω), all continuous frequency dependence of the power spectrum again lies simply and entirely in the denominator of the above expression.
Analogous to Ref [14]'s results, the power-spectrum δ-functions arise from the eigenvalues of T that lie on the unit circle
where ω_{λ} is related to λ by λ=e^{iωλ}. An extension of the Perron–Frobenius theorem guarantees that the eigenvalues of T on the unit circle have index ν_{λ}=1.
Together, these equations yield structural constraints via particular functional forms that are key to solving the inverse problem of inferring process models from measured data.
F. Almost diagonalizable dynamics
The nondiagonalizability that appears most commonly in prediction metadynamics is of a special form that we call almost diagonalizable: when all eigenspaces except one—usually that associated with λ = 0—are diagonalizable subspaces. In the current setting, we say that a matrix is almost diagonalizable if all of its eigenvalues with magnitude greater than zero have geometric multiplicity equal to their algebraic multiplicity.
Definition 1. W is almost diagonalizable if and only if g_{λ}=a_{λ} for all λ∈ΛW∖0≡Λ_{W}∖{0}.
Fortunately, we treat such nondiagonalizability straightforwardly using W^{L}'s spectral decomposition for singular matrices. First off, Eq. (3) simplifies to
Then, to obtain the projection operators associated with each eigenvalue in ΛW∖0 for an almost diagonalizable matrix W, we use Part I's [18] expression for operators with index-one eigenvalues with ν_{λ}=1 for all λ∈ΛW∖0. Finding
for each λ∈ΛW∖0. Or, when more convenient in a calculation, we let ν_{0}→a_{0}−g_{0}+1 or even ν_{0}→a_{0} in Eq. (35), since multiplying W_{λ} by W/λ has no effect.
With the set of projection operators W_{λ} for all λ∈ΛW∖0 in hand, we can use the fact from Part I [18] that projection operators sum to the identity to determine the projection operator associated with the zero eigenvalue
This is sometimes simpler and easier to automate than evaluating W_{0} via the methods of symbolic inversion and residues or via finding all left and right eigenvectors and generalized eigenvectors.
Almost diagonalizable metadynamics play a prominent role in prediction for both processes of a finite Markov order and for the much more general class of processes with broken partial symmetries that can be detected within a finite observation window—the processes of finite symmetry-collapse are discussed next.
G. Markov order versus symmetry collapse
What if zero is the only eigenvalue in the transient structure of a process' MSP? That is, what if there are no loops in the S-MSP transient structure? The associated processes turn out to have finite Markov order.
For processes with finite Markov order R—such as those whose support is a subshift of finite type [15]—the entropy-rate approximates not only converge but also become equal to the true entropy rate when conditioning on long enough histories. Explicitly, for ℓ≥R+1 [12]
or, equivalently, for L≥R
For a finite-order Markov process, all MSP transient states must have identically zero probability after R time steps. The only way to achieve this is if the S-MSP's transient structure is an acyclic directed graph with all probability density flowing away from the unique start-state down to the recurrent component. This means that all eigenvalues associated with the transient states are zero. Moreover, the index of the zero-eigenvalue of the ϵ-machine's S-MSP is equal to the Markov order for finite Markov-order processes. That is, if Λ_{W}∖Λ_{T}={0}, then
In contrast, for stochastic processes whose support is a strictly sofic subshift [15], the Markov order diverges, but ν_{0} can vanish or be finite or infinite. Yet, in either the finite-type or sofic case, ν_{0} still tracks the duration of exact state-space collapse within the transient dynamics of synchronization. This suggests that ν_{0} captures the index of broken symmetries for strictly sofic processes, in analogy to the Markov order for subshifts of finite type. The name symmetry-collapse index captures the essence of ν_{0}'s role in both cases.
Let us explain. In the first ν_{0} time steps, symmetries are broken that synchronize an observer to the process. For the simple period-two process …010101010… the “symmetry” that is broken is the degeneracy of possible phases—the 0 phase or the 1 phase of the period-2 oscillation. Initially, without making a measurement the two phases are indistinguishable. After a single observation, though, the observer learns the phase and is completely synchronized to the process. Hence, ν_{0}=1 for this order-1 Markov process. Simple periodic processes with larger periods have a longer time before the phase information is fully known, hence, their larger Markov order.
For the more complex strictly sofic processes, there may also be symmetries, such as phase information, that are completely broken within a finite amount of time. However, this is only part of the overall transient metadynamics of synchronization. And so, the symmetries completely broken within the symmetry-collapse epoch occur in addition to lingering state uncertainties about a strictly sofic process. As a practical matter, a process' predictability is often substantially enhanced through the finite epoch of symmetry-collapse. This becomes apparent in the examples to follow.
V. SPECTRAL ANALYSIS VIA CORONAL SPECTROGRAMS
Coronal spectrograms are a broadly useful tool in visualizing complexity spectra, from power spectra to excess entropy spectra. They were recently introduced by Ref [14] to demonstrate how diffraction patterns of chaotic crystals emanate from the eigenvalue spectrum of the hidden spatial dynamic of stacked modular layers.
Coronal spectrograms display any frequency-dependent measure f(ω) of a process wrapped around the unit circle while showing the eigenvalues Λ_{T} of the relevant linear dynamic T within the unit circle in the complex plane. Figure 1 (Top) gives an example. This is appropriate for discrete-domain (e.g., discrete-time or discrete-space) dynamics. For continuous-time dynamics, the coronal spectrogram unwraps into what we call the coronated horizon, via the familiar discrete-to-continuous conformal mapping of the inside of the unit circle of the complex plane to the left half of the complex plane [16]. Figure 1 (Bottom) displays a discrete-time version of the coronated horizon. Ultimately, either the coronal spectrogram or coronated horizon yields the same information and lends the same important lesson: the eigenvalues of the hidden linear dynamic control allowed system behaviors.
Coronal spectrograms demonstrate that complex systems behave according to the spectrum of their hidden linear dynamic. The relevant frequency-dependent measure f(ω) emanates from the nonzero eigenvalues of the hidden linear dynamic: the closer eigenvalues approach the unit circle, the sharper the observed peaks. At one extreme, one observes Bragg-like reflections (delta-function contributions) when the eigenvalues fall on the unit circle. The collection of diffuse peaks observed is a sum of Lorentzian-like and, what we might call, super-Lorentzian-like line profiles. Indeed, the Lorentzian-like line profiles are the discrete-time version of a Lorentzian curve. While the continuous-domain Lorentzian is given by Re(cω−λ), the continuous-to-discrete conformal mapping ω→e^{iω} directly yields our discrete-domain analog Re(ce^{iω}−λ). The super-Lorentzian-like line profiles, from nondiagonalizable contributions, have the form Re[(ceiω−λ){n}].
Zero eigenvalues also contribute to f(ω), but only sinusoidal contributions of discrete increments from cos(ω) up to cos(ν_{0}ω). Since these are qualitatively distinct from the super-Lorentzian contributions and do not emanate radially from the eigenvalues the same way contributions from nonzero eigenvalues do, coronal spectrograms are most useful for understanding the contributions of nonzero eigenvalues. Nevertheless, the two contributions can be usefully disentangled, as shown later.
We use both coronal spectrograms and coronated horizons to visualize various features in the examples to follow.
VI. EXAMPLES
A. Golden Mean (GM) Processes
To explore finite Markov order in relation to various complexity measures let us consider the (R−k)-Golden Mean (GM) Processes [17]. This process family describes a unique transition-parametrized process for each Markov-order R∈{ν∈ℤ:ν≥1} and each cryptic-order k∈{κ∈ℤ:1≤κ≤R}. The ϵ-machine for the (4-3)-Golden Mean Process is shown in Fig. 2(a). From this, the construction of all other (R−k)-Golden Mean processes can be discerned. In words, (R−k)-Golden Mean Processes are binary with alphabet A={0,1} and if the most recent history consists of at least k consecutive 0s (and no 1s since then) then there is a probability p of next observing a 1 and a probability 1−p of simply seeing another 0. The first possibility (observing a 1) entails R consecutive 1s followed by at least k consecutive 0s.
The eigenvalues of the internal state-to-state transition matrix of the ϵ-machine's recurrent component are
In the limit of p→1, all (R−k)-Golden Mean Processes become perfectly periodic. In this limit, the eigenvalues are evenly distributed on the unit circle
At the other extreme, as p→0, all eigenvalues evolve to zero, except the stationary eigenvalue at z = 1. At any setting of p, the nonunity eigenvalues lie approximately on a circle within the complex plane whose radius decreases nonlinearly from 1 to 0 as p is swept from 1 to 0. Simultaneously, this circle's center moves from the origin to a positive real value and back to the origin as p is swept from 1 to 0. Figure 3 shows how the eigenvalues of the (5–3)-Golden Mean Process evolve over the full range of p as it sweeps from 1 to 0.
In contrast to the p-dependent spectrum of the recurrent structure just discussed, the only eigenvalue corresponding to the transient structure of the S-MSP is equal to zero, regardless of the transition parameter p. Recall that this is necessarily true for any process with a finite Markov order. Hence, Λ_{W}=Λ_{T}∪{0}, with ν_{0}(W)=R. The cryptic structure is similar: Λ_{ζ}=Λ_{T}∪{0}, with ν_{0}(ζ)=k, where ζ is the state-to-state transition matrix of the cryptic operator presentation.
Figure 4 compares the ϵ-machines, autocorrelation, power spectra, MSPs, myopic entropy rates, and myopic state uncertainties for three p-parametrized examples of (R−k)-GM processes.
The autocorrelation of each process captures their “leaky periodic” behaviors: The leakiness originates from the self-transition at state A that adds a phase-slip noise to otherwise (R+k)–periodic behavior. Moreover, each process' phase, and so its ϵ-machine's internal state, is uniquely identified after R observations. This corresponds to the depth of the S-MSP tree-like structure ν_{0}(W)=R, the convergence of the myopic entropy rate h_{μ}(L) to the true entropy rate h_{μ} when conditioning on observations of finite block-length L−1=R, and the complete loss of causal state uncertainty H(L) after L = R observations.
A paradigm of finite Markov order, the (5–3)-Golden Mean Process has a strictly tree-like structure in its MSP's transients, which have a maximum depth equal to both ν(W)≡ν_{0}(W) and its Markov order of 5.
These analyses illustrate the typical behaviors of complexity measures for finite Markov-order processes. We next investigate examples of infinite Markov-order processes to draw attention to the characteristic differences of nonzero eigenvalues in their MSP transient structures.
B. Even Process
The Even Process, shown in the first column of Fig. 5, is a well known example of a stochastic process that cannot be generated by any finite Markov-order approximation, yet it is generated by a simple two-state HMM.
Infinite Markov order, in this case, stems from the fact that the process generates only an even number of consecutive 1s, between 0s. The countably infinite set of Markov chain states necessary to track this parity reflects the infinite order. Moreover, the surplus entropy rate h_{μ}(L)−h_{μ} incurred when using a finite order-(L−1) Markov approximation vanishes only asymptotically, being the sum of decaying exponentials. (See Fig. 5.) Such long-lived decay is driven by nonzero eigenvalues in the S-MSP transient structure.
This is in stark contrast to the myopic entropy rate for the finite Markov order processes of Fig. 4. For them, h_{μ}(L) drops to h_{μ}exactly at L=R+1. Similarly, the average state uncertainty H(L) for infinite Markov processes converges only asymptotically—and with the same set of decay rates as h_{μ}(L)—to its asymptotic value of 0. (This curve is not shown in Fig. 5 for lack of space.)
The Even Process is a relatively simple example of an infinite Markov-order process. As expected for infinite Markov-order, its MSP's transient structure had nonzero eigenvalues. Generally, though, two ranges of contribution are to be expected in synchronization dynamics. The first is a finite-horizon contribution to the past–future mutual information, corresponding to completely ephemeral zero eigenvalues in the MSP's transient structure. The second is an infinite-horizon contribution to the past–future mutual information, arising from nonzero eigen contributions.
C. Golden–Parity Process Family
To further explore the nature of infinite Markov order processes, we introduce the (ν_{0}−k)-Golden-Parity-(P) Processes. This family subsumes and extends the examples analyzed so far. The role of each parameter is explained in Fig. 2(b), which displays a state-transition diagram of the (4–3)-GP-(3) Process' ϵ-machine.
If P = 1, the family reduces to the (ν_{0}−k)-Golden Mean Process family, with tunable Markov R=ν_{0}(W) and cryptic k orders. That is, (ν_{0}(W)−k)-GP-(1) = (ν_{0}(W)−k)-GM. However, the Markov order becomes infinite whenever P > 1. In this case, the index ν_{0}(W) of the S-MSP's zero-eigenvalue—which controls the finite duration necessary to resolve all broken symmetries—and the cryptic order k can still be tuned independently. The Even Process considered earlier is the (0–0) -Golden-Parity-(2) Process.
Three examples of (ν_{0}(W)−k)-Golden-Parity-(P) processes are analyzed in Fig. 5. The S-MSP transient structure for the second two clarifies the difference between (i) the symmetry collapse associated with completely ephemeral transient states that are fully depleted of probability density after ν_{0}(W) time steps and (ii) the long-lived leaky transients whose probability density only vanishes as more-refined ambiguity is resolved.
Examining the myopic entropy convergence h_{μ}(L), the effect of these distinct routes to synchronization on the predictability can be seen: The process is much more predictable, on average, after ν_{0}(W) time steps. However, the average predictability of an infinite-Markov-order process continues to increase with increasing observation window, albeit with exponentially diminishing returns. In general, we showed that this asymptotic convergence occurs as a sum of decaying exponentials from diagonalizable subspaces and as the product of polynomials and exponentials in the case of nondiagonalizable structures associated with nonzero eigenvalues. The apparent oscillations under the exponential decays are completely described by the leaky periodicities of the eigenvalues in the transient belief states.
Finally, note that the excess entropy spectrum E(ω) shows the frequency domain view of observation-induced predictability. E=limω→0E(ω) is the total past–future mutual information, which is also the excess entropy observed before full synchronization. The ν_{0}(W) symmetry collapse contributes significantly and early to the total excess entropy of the last two examples. However, the asymptotic tails of synchronization associated with leaky periodicity of particular transient states of uncertainty accumulate their contribution to excess entropy rather slowly.
In addition to new intuitions about convergence behaviors in stochastic processes, the general and broadly applicable theoretical results here allow novel numerical investigations and unprecedentedly accurate analyses of infinite-Markov-order processes. As an example of the latter, let us summarize several of the exact results derived in App. A for the (p, q)-parametrized (2–1)-GP-(2) process explored in Fig. 5's second column.
Depending on whether the transition parameter p is larger or smaller than 2q−q, Appendix A found qualitatively distinct behaviors that dominate the (2–1)-GP-(2) process. This hints at a general principle: behaviorally distinct regions are separated by a critical line in the (p, q)-parameter space along which the transition dynamic T becomes nondiagonalizable. For p<2q−q, the autocorrelation for |L|≥2 has the exact solution
where β≡2(p+2q)/(1+p+2q), ζ≡(ξ+1){2}(pξ+2q)/(ξ(ξ^{3}+pξ+2q)), ξ≡−12(p+q)+i124q−(p+q){2}, and ω_{ξ}≡π2+arctan((p+q)/4q−(p+q){2}). The corresponding power spectrum is
For any parameter setting, the metadynamic of observation-induced synchronization to the (2–1)-GP-(2) process is nondiagonalizable due to the index-2 zero eigenvalue. This leads to a completely ephemeral contribution to h_{μ}(L) up to L = 2. For L≥3, we find that the myopic entropy rate relaxes asymptotically to the true entropy rate according to
where the process' true entropy rate is
Interestingly, while the autocorrelation at separation L scales as ∼q^{L/2}, the predictability of transitions between single-symbol-shifted histories of length L converges as ∼p^{L/2}—indicating two rather independent decay rates.
The amount of future that can be predicted from the past is the total mutual information between the observable past and observable future
However, actually performing prediction requires more memory than this amount of shared information. Calculation of additional measures and more details can be found in Appendix A.
To explore the structure in infinite-cryptic order processes, one can use the more generalized family of (ν_{0}(W)−ν_{0}(ζ))-GP-(P−Z) Processes. For them, ν_{0}(ζ) is the index of the zero-eigenvalue of the cryptic operator presentation and the process has an infinite cryptic order whenever Z > 1. Above, Z = 1 and (ν_{0}(W)−ν_{0}(ζ))-GP-(P−1) = (ν_{0}(W)−ν_{0}(ζ))-GP-(P). Since the preceding examples served well enough to illustrate the power of spectral decomposition, our main goal, we leave a full analysis of this family to interested others.
VII. PREDICTING SUPERPAIRWISE STRUCTURE
The Random–Random–XOR (RRXOR) Process is generated by a simple HMM. Figure 6 displays its five-state ϵ-machine. However, it illustrates nontrivial, counterintuitive features typical of stochastic dynamic information processing systems. The process is defined over three steps that repeat: (i) a 0 or 1 is output with equal probability, (ii) another 0 or 1 is output with equal probability, and then (iii) the eXclusive-OR operation (XOR) of the last two outputs is output.
Surprisingly, but calculations easily verify, there are no pairwise correlations. All of its correlations are higher than the second order. One consequence is that its power spectrum is completely flat—the signature of white noise, see Fig. 7. This would lead a casual observer to incorrectly conclude that the generated time series has no structure. In fact, a white noise spectrum is an indication that, if structure is present, it must be hidden in higher-order correlations.
The RRXOR Process clearly is not structureless—via the exclusive OR, it transforms information in a substantial way. We show that the complexity measures introduced above can detect this higher-order structure. However, let us first briefly consider why the correlation-based measures fail to detect the structure in the RRXOR Process.
It is sometimes noted that information measures are superior to standard measures of correlation since they capture nonlinear dependencies, while the standard correlation relies on linear models. And so, we can avoid this problem by using the information correlationI[X_{0};X_{τ}] rather than autocorrelation. Analogous to autocorrelation, it too has a spectral version—the power-of-pairwise information (POPI) spectrum
It is easy to show that I(ω)=0 for the RRXOR Process. Hence, as shown in Fig. 7, such measures are still not sufficient to detect even simple computational structure, since they only can detect pairwise statistical dependencies.
In stark contrast, the excess entropy spectrum E(ω) does identify the structure of hidden dependencies in the RRXOR Process; see Fig. 8. Why? The brief detour through power spectra, information correlation, and POPI spectra brings us to a deeper understanding of why E(ω) is successful at detecting a nuanced computational structure in a time series. Since it partitions all random variables throughout time, the excess entropy itself picks up any systematic influence the past has on the future. The excess entropy spectrum further identifies the frequency decomposition of any such linear or nonlinear dependencies. In short, all multivariate dependencies contribute to the excess entropy spectrum.
Let us now consider the hidden structure of the RRXOR Process in more detail. With reference to Fig. 6, we observe that the expected probability density over causal states evolves through the ϵ-machine with a period-3 modulation. In a given realization, the particular symbols emitted after each phase resetting (ϕ=0) break symmetries with respect to which “wings” of the ϵ-machine structure are traversed. This is reflected in T's eigenvalues: the three roots of unity {e^{in2π/3}}n=02 and two zero eigenvalues, with a_{0}(T)=g_{0}(T)=2 giving index ν_{0}(T)=1.
The period-3 modulation leads to a phase ambiguity when an observer synchronizes to the process—an ambiguity resolved in the MSP transient structure. This resolution is rather complicated, as made explicit in the RRXOR Process' S-MSP, shown in Fig. 9. There are 31 transient states of uncertainty, in addition to the five recurrent states—36 mixed states in total.
Since we derived the ϵ-machine's S-MSP, W=W. Hence, the MSP's layout depicts the information processing involved, while an observer synchronizes to the RRXOR Process. Figure 9's state-transition diagram graphically demonstrates the challenges of optimal prediction, laying out the most direct paths to synchronization allowed by any observation sequence. Falling short of full synchronization—not knowing the process' recurrent state—an observer simply cannot predict the system's future behavior as well as theoretically possible and, so, loses out on any benefits of exact prediction. When synchronized, an optimal observer is surprised only by the irreducible uncertainty h_{μ} bits per observation on average. And, the observer need only monitor b_{μ} bits per observation to stay synchronized. (Recall that b_{μ}, the bound information, is that piece of the irreducible uncertainty h_{μ} that affects a process' future behavior.) While the MSP's transient structure in Fig. 9 shows how synchronization can be attained, its recurrent structure shows how synchronization is maintained.
The MSP introduces new, relevant zero eigenvalues associated with its transient states. In particular, the first-encountered tree-like transients (starting with mixed-state π) introduce new Jordan blocks up to dimension 2. Overall, the 0-eigenspace of W has index 2, so that ν_{0}(W)=2.
Two different sets of leaky-period-3 structures appear in the MSP transients. There are four leaky three-state cycles, each with the same leaky-period-3 contributions to the spectrum: {(14){1/3}e^{in2π/3}}n=02. There are also four leaky four-state cycles, each with a leaky-period-3 contribution and symmetry-breaking 0-eigenvalue contribution to the spectrum: {(12){1/3}e^{in2π/3}}n=02∪{0}. The difference in eigenvalue magnitude, (14){1/3} versus (12){1/3}, implies different timescales of synchronization associated with distinct learning tasks. For example, an immediate lesson is that it takes longer (on average) to escape the 4-state leaky-period-3 components (from the time of arrival) than to escape the preceding 3-state leaky-period-3 components of the synchronizing metadynamic.
The entropy rate convergence plots of Figs. 10 and 11 reveal a sophisticated predictability modulation that simply could not have been gleaned from the spectra of Fig. 7. Figure 11 emphasizes the dominance of the slowest-decaying eigenmodes for large L. Such oscillations under the exponential convergence to synchronization are typical. However, as seen in comparison with Fig. 10 much of the uncertainty may be reduced before this asymptotic mode comes to dominate. Ultimately, synchronization to optimal prediction may involve important contributions from all modes of the mixed-state-to-state metadynamic.
This detailed analysis of the RRXOR Process suggests several general lessons about how we view information in stochastic processes. First, as information processing increases in sophistication, a vanishing amount of a process' intrinsic structure will be discernible at low-orders of correlation. Second, logical computation, as implemented by universal logic gates, primarily operates above pairwise correlation. And so, finally, there is substantial motivation to move beyond measures of pairwise correlation. We must learn to recognize hidden structures and to use higher-order structural investigations to better understand information processing. This is critical to empirically probing functionality in biological and engineered processes.
VIII. CONCLUSION
Surprisingly, many questions we ask about structured stochastic, nonlinear processes implicate a linear dynamic over an appropriate hidden state space. That is, there is an implied hidden Markov model. The promise is that once the dynamic is found for the question of interest, one can make progress in analyzing it. Unfortunately, a roadblock immediately arises: these hidden linear dynamics are generically nondiagonalizable for questions related to prediction and to information and complexity measures. Deploying Part I's [18] meromorphic functional calculus, though, circumvents the roadblock. Using it, we determined closed-form expressions for a very wide range of information and complexity measures. Often, these expressions turned out to be direct functions of the HMM's transition dynamic.
This allowed us to catalog in detail the range of possible convergence behaviors for correlation and myopic uncertainty. The analytical formulas revealed a new symmetry-collapse index that is a lower bound on the Markov order and serves as an independent, more nuanced timescale of information processing. We then considered complexity measures that accumulate during the transient relaxation to observer synchronization. We also introduced the new notion of complexity spectra, gave a new kind of information-theoretic signal analysis in terms of coronal spectrograms, and highlighted common simplifications for special cases, such as almost diagonalizable dynamics. We closed by analyzing several families of finite and infinite Markov and cryptic order processes and emphasized the importance of higher-than-pairwise-order correlations, showing how the excess entropy spectrum is the key diagnostic tool for them.
The analytical completeness might suggest that we have reached an end. Partly, but the truth we seek is rather farther down the road. The meromorphic functional calculus of nondiagonalizable operators merely sets the stage for the next challenges—to develop complexity measures and structural decompositions for infinite-state and infinite excess entropy processes. Hopefully, the new toolset will help us scale the hierarchies of truly complex processes outlined in Refs [1],  [11], and  [12], at a minimum giving exact answers at each stage of a convergent series of finite-ϵ-machine approximations.
The prequel laid out a new toolset that allows one to analyze in detail how complex systems store and process information. Here, we use the tools to calculate in closed form almost all complexity measures for processes generated by finite-state hidden Markov models (HMMs). Helpfully, the tools also give a detailed view of how subprocess components contribute to a process' informational architecture. As an application, we show that the widely used methods based on Fourier analysis and power spectra fail to capture the structure of even very simple structured processes. We introduce the spectrum of past-future mutual information and show that it allows one to detect such structure.
Tracking the evolution of a complex system, a time series of observations often appears quite complicated in the sense of temporal patterns, stochasticity, and behavior that require significant resources to predict. Such complexity arises from many sources. Apparent complexity, even in simple systems, can be induced by practical measurement and analysis issues, such as a small sample size, inadequate collection of probes, noisy or systematically distorted measurements, coarse-graining, out-of-class modeling, nonconvergent inference algorithms, and so on. The effects can either increase or decrease apparent complexity, as they add or discard information, hiding the system of interest from an observer to one degree or another. Assuming perfect observation, complexity can also be inherent in nonlinear stochastic dynamical processes—deterministic chaos, superexponential transients, high state-space dimension, nonergodicity, nonstationarity, and the like. Even in ideal settings, the smallest sufficient set of a system's maximally predictive features is generically uncountable, making approximations unavoidable, in principle [1]. With nothing else said, these facts obviate physical science's most basic goal—prediction—and, without that, they preclude understanding how nature works. How can we make progress?
The prequel, Part I [18], argued that this is too pessimistic a view. It introduced constructive results that address the hidden structure and the challenges associated with predicting complex systems. It showed that questions regarding correlation, predictability, and prediction each require their own analytical structures. Measures of correlation—including autocorrelation functions, power spectra, and Greeen–Kubo transport coefficients—are direct signatures of the transition dynamic of any hidden Markov model (HMM) representation of a process. However, Part I [18] explained that synchronizing to the hidden linear dynamic in systems induces a nondiagonalizable metadynamics, even if the dynamics are diagonalizable in their underlying state-space. Hence, questions about predictability—for example: “How much of the future can be predicted from past observations?” and “What is the irreducible randomness of the process?”—and the burdens of actually predicting—“How much memory must be allocated to predict what is predictable?” and “How much time must be invested before an observer is sufficiently synchronized to make good predictions?”—are instead answered by the nondiagonalizable transition dynamics of the so-called mixed state presentations (MSPs) of the process.
Part I [18] gave operator expressions that begin to answer these questions. However, assuming normal and diagonalizable dynamics, so familiar in mathematical physics, simply fails in this setting. Thus, nondiagonalizable dynamics presented an analytical roadblock. Part I [18] reviewed a calculus for functions of nondiagonalizable operators—the recently developed meromorphic functional calculus of Ref [2]—that directly addresses nondiagonalizability, giving constructive calculational methods and algorithms. Part I [18] also highlighted a spectral weighted directed-graph theory that can give useful shortcuts for determining a process' spectral decomposition.
Part II now goes a step further, providing the first closed-form expressions for many of the complexity measures in wide use. In the following through the derivations, we discover a timescale—the symmetry collapse index—that indicates the sophistication of finite computational structures in infinite-Markov-order processes. We are also led to the introduction of complexity spectra that give a frequency-decomposition of predictable features.
In the following, we consider wide-sense stationary stochastic processes …X_{t−1}X_{t}X_{t+1}… with a discrete domain. The random variable X_{t} can take on values x∈A, which we assume to be a finite set. The interdependencies among the random variables can be interpreted as intrinsic computations carried out by the process under observation. While such stochastic processes are natural in discrete-time signal processing [3,4] and on discrete lattices [5], they are sometimes also obtained from continuous-time chaotic systems via regular sampling. Moreover, while finite alphabets A are intrinsic to certain sources, they may also be obtained from continuous phase-spaces, either by measurement precision limitations or by intentional parsimony as in the case of generating partitions used in symbolic dynamics [6–8]. Such processes are typically non-Markovian and can appear very complicated, but we unravel their complexities via analysis of a HMM representation of the process—where the HMM may be obtained from first principles or may be inferred by any number of methods (see Ref [9] and references therein).
Part II here uses Part I's [18] notation and assumes familiarity with its results.
In Part I [18], we reviewed relevant background in stochastic processes and their complexities and the hidden Markov models (HMMs) that generate them. Part I [18] delineated several classes of HMMs—Markov chains, unifilar HMMs, and nonunifilar HMMs. It also reviewed their mixed-state presentations (MSPs)—HMM generators of a process that track distributions induced by observation. Related constructions included the mixed-functional presentations and cryptic-operator presentations. MSPs are keys to calculating complexity measures within an information-theoretic framing. Part I [18] then showed how each complexity measure reduces to a linear algebra of an appropriate HMM adapted to the cascading- or accumulating-question genre. It summarized the meromorphic functional calculus and several of its mathematical implications in relation to spectral projection operators.
Recall that a distribution η over the states S of an HMM M=(S,A,{T^{(x)}}{x∈A},S_{0}∼η) induces a probability distribution over subsequent words x_{0:L}=x_{0}x_{1}…x_{L−1} generated by the process
where the labeled transition matrices, with elements Ti,j(x)=Pr(X_{t}=x,S_{t+1}=s_{j}|S_{t}=s_{i}), sum to the row-stochastic transition matrix T=∑x∈AT^{(x)} and |1〉 is the all-ones vector such that T|1〉=|1〉.
Ergodic processes will have a single stationary distribution π, such that 〈π|T=〈π|. (Nonergodic processes can still have a specified stationary distribution π over states, but it could no longer be uniquely identified from T alone.) The synchronizing MSP (S-MSP) introduced in Part I [18] is a metadynamic of how distributions over S are updated by observation, starting in the mixed state π and thus starting in the peaked distribution δ_{π} over the set of mixed states. The full set of observation-induced mixed states is R_{π}=∪_{w∈L}{〈π|T^{(w)}〈π|T^{(w)}|1〉}, where L⊂A^{*} is the language of observable words. Although its dynamic is “meta” in relation to the dynamic of the original HMM, the MSP itself is also a HMM with labeled transition operators W^{(x)} summing to the row-stochastic and generically nondiagonalizable mixed-state-to-state transition dynamic W.
Recall that the meromorphic functional calculus prescribes the construction of a new operator from an arbitrary function f of a linear operator A as
where Λ_{A} is the set of A's eigenvalues, ν_{λ} is the index of the eigenvalue λ (i.e., the size of the largest Jordan block associated with λ), z∈ℂ, C_{λ} is a positively oriented Jordan curve around λ that includes no other singularities besides possibly at λ itself, and A_{λ,m}=A_{λ}(λI−A){m} where I is the identity operator and A_{λ} is the spectral projection operator, more simply known as the eigenprojector. The eigenprojector can be constructed as
where |λk(m+1)〉 and 〈λk(m+1)| are the mth generalized right and left eigenvectors, respectively, of the kth Jordan chain such that
and
for 0≤m≤m_{k}−1, where |λj(0)〉=0→ and 〈λj(0)|=0→. Specifically, |λk(1)〉 and 〈λk(1)| are conventional right and left eigenvectors, respectively—these are sometimes written more simply as |λ〉 and 〈λ|, when the algebraic multiplicity a_{λ}=1 and there is thus no risk of ambiguity. After imposing normalization, we find that
Perhaps counterintuitively, this implies for example that the most generalized right eigenvectors are dual to the least generalized left eigenvectors.
Usefully, Part I [18] also demonstrated how eigenvalues can be read off from graph motifs and how eigenvectors and spectral projection operators can be built up hierarchically. Part I [18] should be consulted for further details.
With Part I's [18] toolset laid out, Part II now derives the promised closed-form complexities of a process. Section II investigates the range of possible behaviors for correlation and myopic uncertainty via convergence to asymptotic correlation and asymptotic entropy rates. Section III then considers measures related to accumulating quantities during the transient relaxation to synchronization. Section IV introduces closed-form expressions for a wide range of complexity measures in terms of the spectral decomposition of a process' dynamic. It also introduces complexity spectra and highlights common simplifications for special cases, such as almost diagonalizable dynamics. The excess entropy spectrum should be an especially useful diagnostic tool since it shows not only the extent but also the timescales over which the past can yield predictions about the future. Additionally, we discover a new timescale—the symmetry collapse index—reflective of computational structures in infinite-Markov-order processes. Section V gives a new kind of signal analysis in terms of coronal spectrograms. A suite of examples in Secs. VI and VII ground the theoretical developments and are complemented with an in-depth pedagogical example worked out in Appendix A. Finally, we conclude with a brief retrospective of Parts I and II and turn an eye towards future applications.
Using Part I's [18] methods, our first step is to solve for the autocorrelation function
and the myopic uncertainty or finite-history Shannon entropy rate
While the first of these indicates how a time-series correlates with itself across time, the second indicates how much new information is generated by a single observation beyond what could be inferred from the last L observations. A comparison is informative. We then determine the asymptotic correlation and myopic uncertainty from the resulting finite-L expressions.
A central result in Part I [18] was the spectral decomposition of powers of a linear operator A, even if that operator is nondiagonalizable. Recall that for any L∈ℂ
where (Lm) is the generalized binomial coefficient
(L0)=1, and [0∈Λ_{A}] is the Iverson bracket. The latter takes on value 1 if zero is an eigenvalue of A and 0 if not.
In light of this, the autocorrelation function γ(L) is simply a superposition of weighted eigen-contributions. Part I [18] showed that Eq. (1) has the operator expression
where T is the transition dynamic of any HMM presentation of the process, A is the output symbol alphabet, and we defined the row vector
and the column vector
Substituting Part I's [18] spectral decomposition of matrix powers, Eq. (3), directly leads to the spectral decomposition of γ(L) for nonzero integer L
We denote the persistent first term of Eq. (5) as γ_{⇝}, and note that it can be expressed as
where T^{D} is T's Drazin inverse. We denote the ephemeral second term as γ_{⊸}, which can be written as
where T_{0} is the eigenprojector associated with the eigenvalue of zero; T_{0}=0 if 0∉Λ_{T}.
From Eq. (5), it is now apparent that the index of T's zero eigenvalue gives a finite-horizon contribution (γ_{⊸}) to the autocorrelation function. Beyond index ν_{0} of T, the only L-dependence comes via a weighted sum of terms of the form (|L|−1m)λ^{|L|−1−m}—polynomials in L times decaying exponentials. The set {〈πA¯|T_{λ,m}|A1〉} simply weights the amplitudes of these contributions. In the familiar diagonalizable case, the behavior of autocorrelation is simply a sum of decaying exponentials λ^{|L|}.
Similarly, in light of Part I's [18] expression for the myopic entropy rate in terms of the MSP—starting in the initial unsynchronized mixed-state π and evolving the state of uncertainty via the observation-induced MSP transition dynamic W
where
is simply the column vector whose ith entry is the entropy of transitioning from the ith state of S-MSP—and its spectral decomposition of A^{L}, we find the most general spectral decomposition of the myopic entropy rates h_{μ}(L) to be
We denote the persistent first term of Eq. (8) as h_{⇝}, and note that it can be expressed directly as
where W^{D} is the Drazin inverse of the mixed-state-to-state net transition dynamic W. We denote the ephemeral second term as h_{⊸}, which can be written as
From Eq. (8), we see that the index of W's zero eigenvalue gives a finite horizon contribution (h_{⊸}) to the myopic entropy rate. Beyond index ν_{0} of W, the only L-dependence comes via a weighted sum of terms of the form (L−1m)λ^{L−1−m}—polynomials in L times decaying exponentials. The set {〈δ_{π}|W_{λ,m}|H(W^{A})〉} weighs the amplitudes of these contributions.
For stationary processes we anticipate that for all ζ∈{λ∈Λ_{W}:|λ|=1,λ≠1},〈δ_{π}|W_{ζ}=0 and thus 〈δ_{π}|W_{ζ}|H(W^{A})〉=0. Hence, we can save ourselves from superfluous calculation by excluding the nonunity eigenvalues on the unit circle, when calculating the myopic entropy rate for stationary processes. In the diagonalizable case, again, its behavior is simply a sum of decaying exponentials λ^{L}.
In practice, γ_{⊸} often vanishes, whereas h_{⊸} is often nonzero. This practical difference between γ_{⊸} and h_{⊸} stems from the difference between typical graph structures of the respective dynamics. For a stationary process' generic transition dynamic, zero eigenvalues (and so ν_{0}(T) of T) typically arise from hidden symmetries in the dynamic. In contrast, the MSP of a generic transition dynamic often has tree-like ephemeral structures that are primarily responsible for the zero eigenvalues (and ν_{0}(W)). Nevertheless, despite their practical typical differences, the same mathematical structures appear and contribute to the most general behavior of each of these cascading quantities.
The breadth of qualitative behaviors shared by autocorrelation and the myopic entropy rate is common to the solution of all questions that can be reformulated as a cascading hidden linear dynamic; the myopic state uncertainty H^{+}(L) is just one of many other examples. As we have already seen, however, different measures of a process reflect signatures of different linear operators.
Next, we explore similarities in the qualitative behavior of asymptotics and discuss the implications for correlation and the entropy rate.
The spectral decomposition reveals that the autocorrelation converges to a constant value as L→∞, unless T has eigenvalues on the unit circle besides unity itself. This holds if index ν_{0} is finite, which it is for all processes generated by finite-state HMMs and also many infinite-state HMMs. If unity is the sole eigenvalue with magnitude one, then all other eigenvalues have magnitude less than unity and their contributions vanish for large enough L. Explicitly, if ν_{0}(T)<∞ and argmax_{λ∈ΛT}|λ|={1}, then
This used the fact that ν_{1}=1 and that T_{1}=|1〉〈π| for an ergodic process. This confirms the expectation that 〈XtX¯t+L〉{t}−〈Xt〉{t}〈X¯t〉{t}→0 as L→∞ if argmax_{λ∈ΛT}|λ|={1}, since the random variables (X_{t} and X¯{t+L}) then become de-correlated in the limit of large L.
If other eigenvalues in Λ_{T} besides unity lie on the unit circle, then the autocorrelation approaches a periodic sequence as L gets large. This latter case includes not only deterministically periodic processes, but also stochastic processes with periodic randomness.
By the Perron–Frobenius theorem, ν_{λ}=1 for all eigenvalues of W on the unit circle. Hence, in the limit of L→∞, we obtain the asymptotic entropy rate for any stationary process
since, for stationary processes, 〈δ_{π}|W_{ζ}=0 for all ζ∈{λ∈Λ_{W}:|λ|=1,λ≠1}. For nonstationary processes, the limit may not exist, but h_{μ} may still be found in a suitable sense as a function of time. If the process has only one stationary distribution over mixed states, then W_{1}=|1〉〈π_{W}| and we have
where π_{W} is the stationary distribution over W's states, found either from 〈π_{W}|=〈δ_{π}|W_{1} or from solving 〈π_{W}|W=〈π_{W}|.
A simple but interesting example of when ergodicity does not hold is the multi-armed bandit problem [10,11]. In this, a realization is drawn from an ensemble of differently biased coins or, for that matter, over any other collection of IID processes. More generally, there can be many distinct memoryful stationary components from which a given realization is sampled, according to some probability distribution. With many attracting components we have the stationary mixed-state eigenprojector W_{1}=∑k=1a_{1}|1_{k}〉〈1_{k}|, with 〈1_{j}|1_{k}〉=δ_{j,k}, where the algebraic multiplicity a_{1}(T)=a_{1}(W) of the “1” eigenvalue is the number of attracting components. The entropy rate becomes
Above, 〈δ_{π}|1_{k}〉 is the probability of ending up in component k, while 〈1_{k}|H(W^{A})〉 is component k's entropy rate. Thus, if nonergodic, the process' entropy rate may not be the same as the entropy of any particular realization. Rather, the process' entropy rate is a weighted average of those for the ensemble of sequences constituting the process.
For unifilar M, the topology, transition probabilities, and stationary distribution over the recurrent states are the same for both M and its S-MSP. Hence, for unifilar M we have
One can easily show that Eq. (16) is equivalent to the well-known closed-form expression for h_{μ} for unifilar presentations
For nonunifilar presentations, however, we must use the more general result of Eq. (13). This is similar to the calculation in Eq. (17), but must be performed over the recurrent states of a mixed-state presentation, which may be countable or uncountable.
In the diagonalizable case, autocorrelation, myopic entropy rate, and myopic state uncertainty reduce to a sum of decaying exponentials. Correspondingly, we can find the power spectrum, excess entropy, and synchronization information, respectively, via geometric progressions.
For example, if W is diagonalizable and has no zero eigenvalue, then the myopic entropy rate reduces to
where 〈δ_{π}|W_{1}|H(W^{A})〉 is identifiable as the entropy rate h_{μ}.
It then follows that the excess entropy, which is the mutual information E=I[X←;X→] between the past and the future—and thus how much of the future can be predicted from the past—is
Note that larger eigenvalues (closer to unity magnitude) drive the denominator 1−λ closer to zero and, thus, increase 11−λ. Hence, larger eigenvalues—controlling modes of the mixed-state transition matrix that decay slowly—have the potential to contribute most to excess entropy. Small eigenvalues—quickly decaying modes—do not contribute significantly. Putting aside the language of eigenvalues, one can paraphrase: slowly decaying transient behavior (of the distribution of distributions over process states) has the most potential to make a process appear complex.
Continuing, the transient information, used in the context of synchronization and distinguishing periodic structures [12], is
We now see that the transient information is very closely related to the excess entropy, differing only via the square in the denominators. This comparison between E and T closed-form expressions suggests an entire hierarchy of informational quantities based on eigenvalue weighting.
Performing a similar procedure for the synchronization informationS′ [13] shows that
where |H[η]〉≡∑η∈R_{π}|δ_{η}〉H[η] is the column vector of entropies associated with each mixed-state.
The expressions reveal a remarkably close relationship between S′ and E. Define 〈·|≡∑L=0∞〈δ_{π}|W^{L}. Then
The relationship is now made plain
Although a bit more cumbersome, perhaps better intuition emerges if we rewrite 〈·| as 〈∫Pr(η,L)dL|.
Again, large eigenvalues—slowly decaying modes of the mixed-state transition matrix—can make the largest contribution to synchronization information; small eigenvalues correspond to quickly decaying modes that do not have the opportunity to contribute. In fact, the potential of large eigenvalues to make large contributions is a recurring theme for many questions one has about a process. Simply stated, long-term behavior—what we often interpret as “complex” behavior—is dominated by a process's largest-eigenvalue modes.
That said, a word of warning is in order. Although large-eigenvalue modes have the most potential to make contributions to a process's complexity, the actual set of largest contributors also depends strongly on the amplitudes {〈δ_{π}|W_{λ}|…〉}, where |…〉 is some quantifier vector of interest; e.g., |…〉=|H[η]〉,|…〉=|H(W^{A})〉, or |…〉=|1〉.
Hence, there is as-yet unanticipated similarity between E and T and another between E and S′—at least assuming diagonalizability. We would like to know the relationships between these quantities more generally. However, deriving the general closed-form expressions for accumulated transients is not tractable via the current approach. Rather, to derive the general results, we deploy the meromorphic functional calculus directly at an elevated level, as we now demonstrate.
We now derive the most general closed-form solutions for several complexity measures, from which expressions for related measures follow straightforwardly. This includes an expression for the past–future mutual information or excess entropy, identifying two distinct persistent and transient components, and a novel extension of excess entropy to temporal frequency spectra components. We also give expressions for the synchronization information and power spectra. We explicitly address the class—a common one we argue—of almost diagonalizable dynamics. The section finishes by highlighting finite-order Markov order processes that, rather than being simpler than infinite Markov order processes, introduce technical complications that must be addressed.
Before carrying this out, we define several useful objects. Let ρ(A) be the spectral radius of matrix A
For stochastic W, since ρ(W)=1, let Λ_{ρ(W)} denote the set of eigenvalues with unity magnitude
We also define
and
Eigenvalues with unity magnitude that are not themselves unity correspond to perfectly periodic cycles of the state-transition dynamic. By their very nature, such cycles are restricted to the recurrent states. Moreover, we expect the projection operators associated with these cycles to have no net overlap with the start-state of the MSP. So, we expect
for all λ∈Λ_{ρ(W)}∖{1}. Hence
We will also use the fact that, since ρ(Q)<1
and furthermore
as a consequence of Eq. (21) and our spectral decomposition.
Having seen complexity measures associated with prediction all take on a similar form in terms of the S-MSP state-transition matrix, we expect to encounter similar forms for generically nondiagonalizable state-transition dynamics.
We are now ready to develop the excess entropy in full generality. Our tools turn this into a direct calculation. We find
Note that (I−Q){−1}=inv(I−Q) here, since unity is not an eigenvalue of Q. Indeed, the unity eigenvalue was explicitly extracted from the former matrix to make an invertible expression.
For an ergodic process, where W_{1}=|1〉〈π_{W}|, this becomes
Computationally, Eq. (23) is wonderfully useful. However, the subtraction of h_{μ} is at first mysterious. Especially so, when compared to the compact result for the excess-entropy spectral decomposition in the diagonalizable case given by Eq. (18).
Let us explore this. Recall that Ref [2] showed
for any stochastic matrix T, where T_{1} is the projection operator associated with eigenvalue λ = 1. From this, we see that the general solution for E takes on its most elegant form in terms of the Drazin inverse of I – W
Recall too Part I's [18] explicit spectral decomposition
which uses the companion operatorsT_{λ,m} from there. From this and Eq. (25), we see that the past–future mutual information—the amount of the future that is predictable from the past—has the general spectral decomposition
In light of Eq. (9), we see that there are two qualitatively distinct contributions to the excess entropy E=E_{⇝}+E_{⊸}. One comprises the persistent leaky contributions from all L
and the other is a completely ephemeral piece that contributes only up to W's zero-eigenvalue index ν_{0}
Equation (25) immediately suggests that we generalize the excess entropy, a scalar complexity measure, to a complexity function with continuous part defined in terms of the resolvent—say, via introducing the complex variable z
Such a function not only monitors how much of the future is predictable, but also reveals the time scales of interdependence between the predictable features within the observations. Directly taking the z-transform of h_{μ}(L) comes to mind, but this requires tracking both real and imaginary parts or, alternatively, both the magnitude and complex phase. To ameliorate this, we employ a transform of a closely related function that contains the same information.
Before doing so, we should briefly note that ambiguity surrounds the appropriate excess-entropy generalization. There are many alternate measures that approach the excess entropy as frequency goes to zero. For example, directly calculating from the meromorphic functional calculus, letting z=e^{iω} we find
We are challenged, however, to interpret the fact that Re〈δ_{π}|(eiωI−W){−1}|H(W^{A})〉+12h_{μ} is not necessarily positive at all frequencies. Another direct calculation shows that
Enticingly, Re〈δ_{π}|e^{iω}(eiωI−W){−1}|H(W^{A})〉−12h_{μ} appears to be positive over all frequencies for all examples checked. It is not immediately clear which, if either, is the appropriate generalization, though. Fortunately, the Fourier transform of a two-sided myopic-entropy convergence function makes our upcoming definition of E(ω) interpretable and of interest in its own right.
Let  be the two-sided myopic entropy convergence function defined by
For stationary processes, it is easy to show that H[X_{0}|X_{−L+1:0}]=H[X_{0}|X_{1:L}], with the result that  is a symmetric function. Moreover, h then simplifies to
where h_{μ}(0)≡log_{2}|A| and, as before, h_{μ}(L)=H[X_{L}|X_{1:L}] for L≥1 with h_{μ}(1)=H[X_{1}].
The symmetry of the two-sided myopic entropy convergence function h guarantees that its Fourier transform is also real and symmetric. Explicitly, the continuous part of the Fourier transform turns out to be
a strictly real and symmetric function of the angular frequency ω. Here, R is the redundancy of the alphabet R≡log_{2}|A|−h_{μ}, as in Ref [12].
The transform  also has a discrete impulsive component. For stationary processes, this consists solely of the Dirac δ-function at zero frequency
Recall that the Fourier transform of a discrete-domain function is 2π-periodic in the angular frequency ω. This δ-function is associated with the nonzero offset of the entropy convergence curve of positive-entropy-rate processes. The full transform is
Direct calculation using Ref [2]'s meromorphic functional calculus shows that
This motivates introducing the excess-entropy spectrumE(ω)
The excess-entropy spectrum rather directly displays important frequencies of apparent entropy reduction. For example, leaky period-5 processes have a period-5 signature in the excess entropy spectrum.
As with its predecessors, the excess-entropy spectrum also has a natural decomposition into two qualitatively distinct components
The excess-entropy spectrum gives an intuitive and concise summary of the complexities associated with a process' predictability. For example, given a graph of the excess entropy spectrum, the past–future mutual information can be read off as the height of the continuous part of the function as it approaches zero frequency
Indeed, the limit of zero frequency is necessary due to the δ-function in the Fourier transform at exactly zero frequency
Reflecting on this, the δ-function indicates one of the reasons the excess entropy has been difficult to compute in the past. This also sheds light on the role of the Drazin inverse: It removes the infinite asymptotic accumulation, revealing the transient structure of entropy convergence.
We also have a spectral decomposition of the excess-entropy spectrum
where, in the last equality, we assume that W_{0} is real. This shows that, in addition to the contribution of typical leaky modes of decay in entropy convergence, the zero-eigenvalue modes contribute uniquely to the excess entropy spectrum. In addition to Lorentzian-like spectral curves contributed by leaky periodicities in the MSP, the excess-entropy spectrum also contains sums of cosines up to a frequency controlled by index ν_{0}, which corresponds to the depth of the MSP's nondiagonalizability. This is simply the duration of ephemeral synchronization in the time domain.
Once expressed in terms of the S-MSP transition dynamic, the derivation of the excess synchronization information S′ closely parallels that of the excess entropy, only with a different ket |·〉 appended. We calculate, as before, finding
For an ergodic process where W_{1}=|1〉〈π_{W}|, this becomes
From Eq. (24), we see that the general solution for S′ takes on its most elegant form in terms of the Drazin inverse of I – W
From Eqs. (32) and (26), we also see that the excess synchronization information has the general spectral decomposition
Again the form of Eq. (32) suggests generalizing synchronization information from a complexity measure to a complexity function S(ω). In this case, the result is simply related to the Fourier transform of the two-sided myopic state-uncertainty H(L).
The extended complexity functions, E(ω) and S(ω) just introduced, give the same intuitive understanding for entropy reduction and synchronization, respectively, as the power spectrum P(ω) gives for pairwise correlation. Recall from Part I [18] that the power spectrum can be written as
We see that (eiωI−T){−1} is the resolvent of T evaluated along the unit circle z=e^{iω} for ω∈[0,2π). Hence, by Part I's [18] decomposition of the resolvent, the general spectral decomposition of the continuous part of the power spectrum is
As with E(ω) and S(ω), all continuous frequency dependence of the power spectrum again lies simply and entirely in the denominator of the above expression.
Analogous to Ref [14]'s results, the power-spectrum δ-functions arise from the eigenvalues of T that lie on the unit circle
where ω_{λ} is related to λ by λ=e^{iωλ}. An extension of the Perron–Frobenius theorem guarantees that the eigenvalues of T on the unit circle have index ν_{λ}=1.
Together, these equations yield structural constraints via particular functional forms that are key to solving the inverse problem of inferring process models from measured data.
The nondiagonalizability that appears most commonly in prediction metadynamics is of a special form that we call almost diagonalizable: when all eigenspaces except one—usually that associated with λ = 0—are diagonalizable subspaces. In the current setting, we say that a matrix is almost diagonalizable if all of its eigenvalues with magnitude greater than zero have geometric multiplicity equal to their algebraic multiplicity.
Definition 1. W is almost diagonalizable if and only if g_{λ}=a_{λ} for all λ∈ΛW∖0≡Λ_{W}∖{0}.
Fortunately, we treat such nondiagonalizability straightforwardly using W^{L}'s spectral decomposition for singular matrices. First off, Eq. (3) simplifies to
Then, to obtain the projection operators associated with each eigenvalue in ΛW∖0 for an almost diagonalizable matrix W, we use Part I's [18] expression for operators with index-one eigenvalues with ν_{λ}=1 for all λ∈ΛW∖0. Finding
for each λ∈ΛW∖0. Or, when more convenient in a calculation, we let ν_{0}→a_{0}−g_{0}+1 or even ν_{0}→a_{0} in Eq. (35), since multiplying W_{λ} by W/λ has no effect.
With the set of projection operators W_{λ} for all λ∈ΛW∖0 in hand, we can use the fact from Part I [18] that projection operators sum to the identity to determine the projection operator associated with the zero eigenvalue
This is sometimes simpler and easier to automate than evaluating W_{0} via the methods of symbolic inversion and residues or via finding all left and right eigenvectors and generalized eigenvectors.
Almost diagonalizable metadynamics play a prominent role in prediction for both processes of a finite Markov order and for the much more general class of processes with broken partial symmetries that can be detected within a finite observation window—the processes of finite symmetry-collapse are discussed next.
What if zero is the only eigenvalue in the transient structure of a process' MSP? That is, what if there are no loops in the S-MSP transient structure? The associated processes turn out to have finite Markov order.
For processes with finite Markov order R—such as those whose support is a subshift of finite type [15]—the entropy-rate approximates not only converge but also become equal to the true entropy rate when conditioning on long enough histories. Explicitly, for ℓ≥R+1 [12]
or, equivalently, for L≥R
For a finite-order Markov process, all MSP transient states must have identically zero probability after R time steps. The only way to achieve this is if the S-MSP's transient structure is an acyclic directed graph with all probability density flowing away from the unique start-state down to the recurrent component. This means that all eigenvalues associated with the transient states are zero. Moreover, the index of the zero-eigenvalue of the ϵ-machine's S-MSP is equal to the Markov order for finite Markov-order processes. That is, if Λ_{W}∖Λ_{T}={0}, then
In contrast, for stochastic processes whose support is a strictly sofic subshift [15], the Markov order diverges, but ν_{0} can vanish or be finite or infinite. Yet, in either the finite-type or sofic case, ν_{0} still tracks the duration of exact state-space collapse within the transient dynamics of synchronization. This suggests that ν_{0} captures the index of broken symmetries for strictly sofic processes, in analogy to the Markov order for subshifts of finite type. The name symmetry-collapse index captures the essence of ν_{0}'s role in both cases.
Let us explain. In the first ν_{0} time steps, symmetries are broken that synchronize an observer to the process. For the simple period-two process …010101010… the “symmetry” that is broken is the degeneracy of possible phases—the 0 phase or the 1 phase of the period-2 oscillation. Initially, without making a measurement the two phases are indistinguishable. After a single observation, though, the observer learns the phase and is completely synchronized to the process. Hence, ν_{0}=1 for this order-1 Markov process. Simple periodic processes with larger periods have a longer time before the phase information is fully known, hence, their larger Markov order.
For the more complex strictly sofic processes, there may also be symmetries, such as phase information, that are completely broken within a finite amount of time. However, this is only part of the overall transient metadynamics of synchronization. And so, the symmetries completely broken within the symmetry-collapse epoch occur in addition to lingering state uncertainties about a strictly sofic process. As a practical matter, a process' predictability is often substantially enhanced through the finite epoch of symmetry-collapse. This becomes apparent in the examples to follow.
Coronal spectrograms are a broadly useful tool in visualizing complexity spectra, from power spectra to excess entropy spectra. They were recently introduced by Ref [14] to demonstrate how diffraction patterns of chaotic crystals emanate from the eigenvalue spectrum of the hidden spatial dynamic of stacked modular layers.
Coronal spectrograms display any frequency-dependent measure f(ω) of a process wrapped around the unit circle while showing the eigenvalues Λ_{T} of the relevant linear dynamic T within the unit circle in the complex plane. Figure 1 (Top) gives an example. This is appropriate for discrete-domain (e.g., discrete-time or discrete-space) dynamics. For continuous-time dynamics, the coronal spectrogram unwraps into what we call the coronated horizon, via the familiar discrete-to-continuous conformal mapping of the inside of the unit circle of the complex plane to the left half of the complex plane [16]. Figure 1 (Bottom) displays a discrete-time version of the coronated horizon. Ultimately, either the coronal spectrogram or coronated horizon yields the same information and lends the same important lesson: the eigenvalues of the hidden linear dynamic control allowed system behaviors.
Coronal spectrograms demonstrate that complex systems behave according to the spectrum of their hidden linear dynamic. The relevant frequency-dependent measure f(ω) emanates from the nonzero eigenvalues of the hidden linear dynamic: the closer eigenvalues approach the unit circle, the sharper the observed peaks. At one extreme, one observes Bragg-like reflections (delta-function contributions) when the eigenvalues fall on the unit circle. The collection of diffuse peaks observed is a sum of Lorentzian-like and, what we might call, super-Lorentzian-like line profiles. Indeed, the Lorentzian-like line profiles are the discrete-time version of a Lorentzian curve. While the continuous-domain Lorentzian is given by Re(cω−λ), the continuous-to-discrete conformal mapping ω→e^{iω} directly yields our discrete-domain analog Re(ce^{iω}−λ). The super-Lorentzian-like line profiles, from nondiagonalizable contributions, have the form Re[(ceiω−λ){n}].
Zero eigenvalues also contribute to f(ω), but only sinusoidal contributions of discrete increments from cos(ω) up to cos(ν_{0}ω). Since these are qualitatively distinct from the super-Lorentzian contributions and do not emanate radially from the eigenvalues the same way contributions from nonzero eigenvalues do, coronal spectrograms are most useful for understanding the contributions of nonzero eigenvalues. Nevertheless, the two contributions can be usefully disentangled, as shown later.
We use both coronal spectrograms and coronated horizons to visualize various features in the examples to follow.
To explore finite Markov order in relation to various complexity measures let us consider the (R−k)-Golden Mean (GM) Processes [17]. This process family describes a unique transition-parametrized process for each Markov-order R∈{ν∈ℤ:ν≥1} and each cryptic-order k∈{κ∈ℤ:1≤κ≤R}. The ϵ-machine for the (4-3)-Golden Mean Process is shown in Fig. 2(a). From this, the construction of all other (R−k)-Golden Mean processes can be discerned. In words, (R−k)-Golden Mean Processes are binary with alphabet A={0,1} and if the most recent history consists of at least k consecutive 0s (and no 1s since then) then there is a probability p of next observing a 1 and a probability 1−p of simply seeing another 0. The first possibility (observing a 1) entails R consecutive 1s followed by at least k consecutive 0s.
The eigenvalues of the internal state-to-state transition matrix of the ϵ-machine's recurrent component are
In the limit of p→1, all (R−k)-Golden Mean Processes become perfectly periodic. In this limit, the eigenvalues are evenly distributed on the unit circle
At the other extreme, as p→0, all eigenvalues evolve to zero, except the stationary eigenvalue at z = 1. At any setting of p, the nonunity eigenvalues lie approximately on a circle within the complex plane whose radius decreases nonlinearly from 1 to 0 as p is swept from 1 to 0. Simultaneously, this circle's center moves from the origin to a positive real value and back to the origin as p is swept from 1 to 0. Figure 3 shows how the eigenvalues of the (5–3)-Golden Mean Process evolve over the full range of p as it sweeps from 1 to 0.
In contrast to the p-dependent spectrum of the recurrent structure just discussed, the only eigenvalue corresponding to the transient structure of the S-MSP is equal to zero, regardless of the transition parameter p. Recall that this is necessarily true for any process with a finite Markov order. Hence, Λ_{W}=Λ_{T}∪{0}, with ν_{0}(W)=R. The cryptic structure is similar: Λ_{ζ}=Λ_{T}∪{0}, with ν_{0}(ζ)=k, where ζ is the state-to-state transition matrix of the cryptic operator presentation.
Figure 4 compares the ϵ-machines, autocorrelation, power spectra, MSPs, myopic entropy rates, and myopic state uncertainties for three p-parametrized examples of (R−k)-GM processes.
The autocorrelation of each process captures their “leaky periodic” behaviors: The leakiness originates from the self-transition at state A that adds a phase-slip noise to otherwise (R+k)–periodic behavior. Moreover, each process' phase, and so its ϵ-machine's internal state, is uniquely identified after R observations. This corresponds to the depth of the S-MSP tree-like structure ν_{0}(W)=R, the convergence of the myopic entropy rate h_{μ}(L) to the true entropy rate h_{μ} when conditioning on observations of finite block-length L−1=R, and the complete loss of causal state uncertainty H(L) after L = R observations.
A paradigm of finite Markov order, the (5–3)-Golden Mean Process has a strictly tree-like structure in its MSP's transients, which have a maximum depth equal to both ν(W)≡ν_{0}(W) and its Markov order of 5.
These analyses illustrate the typical behaviors of complexity measures for finite Markov-order processes. We next investigate examples of infinite Markov-order processes to draw attention to the characteristic differences of nonzero eigenvalues in their MSP transient structures.
The Even Process, shown in the first column of Fig. 5, is a well known example of a stochastic process that cannot be generated by any finite Markov-order approximation, yet it is generated by a simple two-state HMM.
Infinite Markov order, in this case, stems from the fact that the process generates only an even number of consecutive 1s, between 0s. The countably infinite set of Markov chain states necessary to track this parity reflects the infinite order. Moreover, the surplus entropy rate h_{μ}(L)−h_{μ} incurred when using a finite order-(L−1) Markov approximation vanishes only asymptotically, being the sum of decaying exponentials. (See Fig. 5.) Such long-lived decay is driven by nonzero eigenvalues in the S-MSP transient structure.
This is in stark contrast to the myopic entropy rate for the finite Markov order processes of Fig. 4. For them, h_{μ}(L) drops to h_{μ}exactly at L=R+1. Similarly, the average state uncertainty H(L) for infinite Markov processes converges only asymptotically—and with the same set of decay rates as h_{μ}(L)—to its asymptotic value of 0. (This curve is not shown in Fig. 5 for lack of space.)
The Even Process is a relatively simple example of an infinite Markov-order process. As expected for infinite Markov-order, its MSP's transient structure had nonzero eigenvalues. Generally, though, two ranges of contribution are to be expected in synchronization dynamics. The first is a finite-horizon contribution to the past–future mutual information, corresponding to completely ephemeral zero eigenvalues in the MSP's transient structure. The second is an infinite-horizon contribution to the past–future mutual information, arising from nonzero eigen contributions.
To further explore the nature of infinite Markov order processes, we introduce the (ν_{0}−k)-Golden-Parity-(P) Processes. This family subsumes and extends the examples analyzed so far. The role of each parameter is explained in Fig. 2(b), which displays a state-transition diagram of the (4–3)-GP-(3) Process' ϵ-machine.
If P = 1, the family reduces to the (ν_{0}−k)-Golden Mean Process family, with tunable Markov R=ν_{0}(W) and cryptic k orders. That is, (ν_{0}(W)−k)-GP-(1) = (ν_{0}(W)−k)-GM. However, the Markov order becomes infinite whenever P > 1. In this case, the index ν_{0}(W) of the S-MSP's zero-eigenvalue—which controls the finite duration necessary to resolve all broken symmetries—and the cryptic order k can still be tuned independently. The Even Process considered earlier is the (0–0) -Golden-Parity-(2) Process.
Three examples of (ν_{0}(W)−k)-Golden-Parity-(P) processes are analyzed in Fig. 5. The S-MSP transient structure for the second two clarifies the difference between (i) the symmetry collapse associated with completely ephemeral transient states that are fully depleted of probability density after ν_{0}(W) time steps and (ii) the long-lived leaky transients whose probability density only vanishes as more-refined ambiguity is resolved.
Examining the myopic entropy convergence h_{μ}(L), the effect of these distinct routes to synchronization on the predictability can be seen: The process is much more predictable, on average, after ν_{0}(W) time steps. However, the average predictability of an infinite-Markov-order process continues to increase with increasing observation window, albeit with exponentially diminishing returns. In general, we showed that this asymptotic convergence occurs as a sum of decaying exponentials from diagonalizable subspaces and as the product of polynomials and exponentials in the case of nondiagonalizable structures associated with nonzero eigenvalues. The apparent oscillations under the exponential decays are completely described by the leaky periodicities of the eigenvalues in the transient belief states.
Finally, note that the excess entropy spectrum E(ω) shows the frequency domain view of observation-induced predictability. E=limω→0E(ω) is the total past–future mutual information, which is also the excess entropy observed before full synchronization. The ν_{0}(W) symmetry collapse contributes significantly and early to the total excess entropy of the last two examples. However, the asymptotic tails of synchronization associated with leaky periodicity of particular transient states of uncertainty accumulate their contribution to excess entropy rather slowly.
In addition to new intuitions about convergence behaviors in stochastic processes, the general and broadly applicable theoretical results here allow novel numerical investigations and unprecedentedly accurate analyses of infinite-Markov-order processes. As an example of the latter, let us summarize several of the exact results derived in App. A for the (p, q)-parametrized (2–1)-GP-(2) process explored in Fig. 5's second column.
Depending on whether the transition parameter p is larger or smaller than 2q−q, Appendix A found qualitatively distinct behaviors that dominate the (2–1)-GP-(2) process. This hints at a general principle: behaviorally distinct regions are separated by a critical line in the (p, q)-parameter space along which the transition dynamic T becomes nondiagonalizable. For p<2q−q, the autocorrelation for |L|≥2 has the exact solution
where β≡2(p+2q)/(1+p+2q), ζ≡(ξ+1){2}(pξ+2q)/(ξ(ξ^{3}+pξ+2q)), ξ≡−12(p+q)+i124q−(p+q){2}, and ω_{ξ}≡π2+arctan((p+q)/4q−(p+q){2}). The corresponding power spectrum is
For any parameter setting, the metadynamic of observation-induced synchronization to the (2–1)-GP-(2) process is nondiagonalizable due to the index-2 zero eigenvalue. This leads to a completely ephemeral contribution to h_{μ}(L) up to L = 2. For L≥3, we find that the myopic entropy rate relaxes asymptotically to the true entropy rate according to
where the process' true entropy rate is
Interestingly, while the autocorrelation at separation L scales as ∼q^{L/2}, the predictability of transitions between single-symbol-shifted histories of length L converges as ∼p^{L/2}—indicating two rather independent decay rates.
The amount of future that can be predicted from the past is the total mutual information between the observable past and observable future
However, actually performing prediction requires more memory than this amount of shared information. Calculation of additional measures and more details can be found in Appendix A.
To explore the structure in infinite-cryptic order processes, one can use the more generalized family of (ν_{0}(W)−ν_{0}(ζ))-GP-(P−Z) Processes. For them, ν_{0}(ζ) is the index of the zero-eigenvalue of the cryptic operator presentation and the process has an infinite cryptic order whenever Z > 1. Above, Z = 1 and (ν_{0}(W)−ν_{0}(ζ))-GP-(P−1) = (ν_{0}(W)−ν_{0}(ζ))-GP-(P). Since the preceding examples served well enough to illustrate the power of spectral decomposition, our main goal, we leave a full analysis of this family to interested others.
The Random–Random–XOR (RRXOR) Process is generated by a simple HMM. Figure 6 displays its five-state ϵ-machine. However, it illustrates nontrivial, counterintuitive features typical of stochastic dynamic information processing systems. The process is defined over three steps that repeat: (i) a 0 or 1 is output with equal probability, (ii) another 0 or 1 is output with equal probability, and then (iii) the eXclusive-OR operation (XOR) of the last two outputs is output.
Surprisingly, but calculations easily verify, there are no pairwise correlations. All of its correlations are higher than the second order. One consequence is that its power spectrum is completely flat—the signature of white noise, see Fig. 7. This would lead a casual observer to incorrectly conclude that the generated time series has no structure. In fact, a white noise spectrum is an indication that, if structure is present, it must be hidden in higher-order correlations.
The RRXOR Process clearly is not structureless—via the exclusive OR, it transforms information in a substantial way. We show that the complexity measures introduced above can detect this higher-order structure. However, let us first briefly consider why the correlation-based measures fail to detect the structure in the RRXOR Process.
It is sometimes noted that information measures are superior to standard measures of correlation since they capture nonlinear dependencies, while the standard correlation relies on linear models. And so, we can avoid this problem by using the information correlationI[X_{0};X_{τ}] rather than autocorrelation. Analogous to autocorrelation, it too has a spectral version—the power-of-pairwise information (POPI) spectrum
It is easy to show that I(ω)=0 for the RRXOR Process. Hence, as shown in Fig. 7, such measures are still not sufficient to detect even simple computational structure, since they only can detect pairwise statistical dependencies.
In stark contrast, the excess entropy spectrum E(ω) does identify the structure of hidden dependencies in the RRXOR Process; see Fig. 8. Why? The brief detour through power spectra, information correlation, and POPI spectra brings us to a deeper understanding of why E(ω) is successful at detecting a nuanced computational structure in a time series. Since it partitions all random variables throughout time, the excess entropy itself picks up any systematic influence the past has on the future. The excess entropy spectrum further identifies the frequency decomposition of any such linear or nonlinear dependencies. In short, all multivariate dependencies contribute to the excess entropy spectrum.
Let us now consider the hidden structure of the RRXOR Process in more detail. With reference to Fig. 6, we observe that the expected probability density over causal states evolves through the ϵ-machine with a period-3 modulation. In a given realization, the particular symbols emitted after each phase resetting (ϕ=0) break symmetries with respect to which “wings” of the ϵ-machine structure are traversed. This is reflected in T's eigenvalues: the three roots of unity {e^{in2π/3}}n=02 and two zero eigenvalues, with a_{0}(T)=g_{0}(T)=2 giving index ν_{0}(T)=1.
The period-3 modulation leads to a phase ambiguity when an observer synchronizes to the process—an ambiguity resolved in the MSP transient structure. This resolution is rather complicated, as made explicit in the RRXOR Process' S-MSP, shown in Fig. 9. There are 31 transient states of uncertainty, in addition to the five recurrent states—36 mixed states in total.
Since we derived the ϵ-machine's S-MSP, W=W. Hence, the MSP's layout depicts the information processing involved, while an observer synchronizes to the RRXOR Process. Figure 9's state-transition diagram graphically demonstrates the challenges of optimal prediction, laying out the most direct paths to synchronization allowed by any observation sequence. Falling short of full synchronization—not knowing the process' recurrent state—an observer simply cannot predict the system's future behavior as well as theoretically possible and, so, loses out on any benefits of exact prediction. When synchronized, an optimal observer is surprised only by the irreducible uncertainty h_{μ} bits per observation on average. And, the observer need only monitor b_{μ} bits per observation to stay synchronized. (Recall that b_{μ}, the bound information, is that piece of the irreducible uncertainty h_{μ} that affects a process' future behavior.) While the MSP's transient structure in Fig. 9 shows how synchronization can be attained, its recurrent structure shows how synchronization is maintained.
The MSP introduces new, relevant zero eigenvalues associated with its transient states. In particular, the first-encountered tree-like transients (starting with mixed-state π) introduce new Jordan blocks up to dimension 2. Overall, the 0-eigenspace of W has index 2, so that ν_{0}(W)=2.
Two different sets of leaky-period-3 structures appear in the MSP transients. There are four leaky three-state cycles, each with the same leaky-period-3 contributions to the spectrum: {(14){1/3}e^{in2π/3}}n=02. There are also four leaky four-state cycles, each with a leaky-period-3 contribution and symmetry-breaking 0-eigenvalue contribution to the spectrum: {(12){1/3}e^{in2π/3}}n=02∪{0}. The difference in eigenvalue magnitude, (14){1/3} versus (12){1/3}, implies different timescales of synchronization associated with distinct learning tasks. For example, an immediate lesson is that it takes longer (on average) to escape the 4-state leaky-period-3 components (from the time of arrival) than to escape the preceding 3-state leaky-period-3 components of the synchronizing metadynamic.
The entropy rate convergence plots of Figs. 10 and 11 reveal a sophisticated predictability modulation that simply could not have been gleaned from the spectra of Fig. 7. Figure 11 emphasizes the dominance of the slowest-decaying eigenmodes for large L. Such oscillations under the exponential convergence to synchronization are typical. However, as seen in comparison with Fig. 10 much of the uncertainty may be reduced before this asymptotic mode comes to dominate. Ultimately, synchronization to optimal prediction may involve important contributions from all modes of the mixed-state-to-state metadynamic.
This detailed analysis of the RRXOR Process suggests several general lessons about how we view information in stochastic processes. First, as information processing increases in sophistication, a vanishing amount of a process' intrinsic structure will be discernible at low-orders of correlation. Second, logical computation, as implemented by universal logic gates, primarily operates above pairwise correlation. And so, finally, there is substantial motivation to move beyond measures of pairwise correlation. We must learn to recognize hidden structures and to use higher-order structural investigations to better understand information processing. This is critical to empirically probing functionality in biological and engineered processes.
Surprisingly, many questions we ask about structured stochastic, nonlinear processes implicate a linear dynamic over an appropriate hidden state space. That is, there is an implied hidden Markov model. The promise is that once the dynamic is found for the question of interest, one can make progress in analyzing it. Unfortunately, a roadblock immediately arises: these hidden linear dynamics are generically nondiagonalizable for questions related to prediction and to information and complexity measures. Deploying Part I's [18] meromorphic functional calculus, though, circumvents the roadblock. Using it, we determined closed-form expressions for a very wide range of information and complexity measures. Often, these expressions turned out to be direct functions of the HMM's transition dynamic.
This allowed us to catalog in detail the range of possible convergence behaviors for correlation and myopic uncertainty. The analytical formulas revealed a new symmetry-collapse index that is a lower bound on the Markov order and serves as an independent, more nuanced timescale of information processing. We then considered complexity measures that accumulate during the transient relaxation to observer synchronization. We also introduced the new notion of complexity spectra, gave a new kind of information-theoretic signal analysis in terms of coronal spectrograms, and highlighted common simplifications for special cases, such as almost diagonalizable dynamics. We closed by analyzing several families of finite and infinite Markov and cryptic order processes and emphasized the importance of higher-than-pairwise-order correlations, showing how the excess entropy spectrum is the key diagnostic tool for them.
The analytical completeness might suggest that we have reached an end. Partly, but the truth we seek is rather farther down the road. The meromorphic functional calculus of nondiagonalizable operators merely sets the stage for the next challenges—to develop complexity measures and structural decompositions for infinite-state and infinite excess entropy processes. Hopefully, the new toolset will help us scale the hierarchies of truly complex processes outlined in Refs [1],  [11], and  [12], at a minimum giving exact answers at each stage of a convergent series of finite-ϵ-machine approximations.
APPENDIX A: EXAMPLE ANALYTICAL CALCULATIONS
The (p, q)-parametrized (2-1)-GP-(2) Process is described by its ϵ-machine, whose state-transition diagram was shown in the first row and second column of Fig. 5 and is reproduced here in Fig. 12. Formally, the (2-1)-GP-(2) stationary stochastic process is generated by the HMM M_{ϵM}=(S,A,{T^{(x)}}{x∈A},η_{0}=π). That is, M consists of a set of hidden causal states S={A,B,C,D}, an alphabet A={0,1,2} of symbols emitted to form the observed process, and a set {T^{(x)}:Ts,s^{′}(x)=Pr(Xt=x,St+1=s′|St=s)}{x∈A} of symbol-labeled transition matrices. These are
and
The symbol-labeled transition matrices sum to the row-stochastic internal state-to-state transition matrix
This is a Markov chain over the hidden states. From it, we find the stationary state distribution 〈π|=〈π|T
The first analysis task is to determine the eigenvalues and the associated projection operators for the internal state-to-state transition matrix T. From
we find the four eigenvalues
All eigenvalues are real for p≥2q−q. Two are complex with a nonzero imaginary part when p<2q−q. Putting this together with the transition-probability consistency constraint that p+q<1 yields the map of the transition-parameter space shown in Fig. 13.
For a generic choice of the parameter setting (p, q), all T's eigenvalues are unique, and so T is diagonalizable. However, two of the eigenvalues become degenerate along the parameter subspace p=2q−q, giving Λ_{T}={1,0,−q}. We find that the algebraic multiplicity a_{−q}=2 is larger than the geometric multiplicity g_{−q}=1, yielding nondiagonalizability (ν_{−q}=2) along this (p=2q−q)-submanifold. More broadly, experience has shown that eigenvalues generically induce nondiagonalizability when they collide and scatter in the complex plane. For example, this occurs when a pair of eigenvalues first “entangle” to become complex conjugate pairs.
For any parameter setting (p, q), we find T's right eigenvectors from (T−λI)|λ〉=0→. They are
and
for all λ∈Λ_{T}∖{0}. Similarly, we find the left eigenvectors of T from 〈λ|(T−λI)=0→
and
for all λ∈Λ_{T}∖{0}. Clearly, the left eigenvectors are not simply the complex-conjugate transpose of the right eigenvectors. This is a signature of the more intricate algebraic structure in these processes.
Since T is generically diagonalizable, all of the projection operators are simply the normalized ket-bra outer products
so long as p≠2q−q. To wit
Along the nondiagonalizable (p=2q−q)-subspace of parameter settings: 〈−q|−q〉=0. This corresponds to the fact that the right and left eigenvectors are now dual to the left and right generalized eigenvectors, rather than being dual to each other. We find that the projection operator for the nondiagonalizable eigenspace is
where
Equivalently, T_{−q}=I−T_{1}−T_{0}. The projection operators for the remaining (λ=0,1) eigenspaces retain the same form as before: T_{λ}=|λ〉〈λ|/〈λ|λ〉.
The pieces are now in place to calculate the observable correlation and power spectrum. Recall that we derived the general spectral decomposition of the autocorrelation function γ(L)=〈X¯tXt+L〉{t}
for nonzero integer L, where
and
For generic parameter settings, this reduces to
Notably, the autocorrelation splits into an ephemeral part (via the Kronecker delta) due to the zero eigenvalue, an exponentially decaying oscillatory part due to the two eigenvalues with magnitude between 0 and 1, and an asymptotic part that survives even as L→∞ due to the eigenvalue of unity. Moreover, for p<2q−q, the two nontrivial eigenvalues become complex conjugate pairs. This allows us to rewrite the autocorrelation for the (2-1)-GP-(2) process for |L|≥2 concisely as
where
The latter form reveals that the magnitude of the largest nonunity eigenvalue |ξ|=q controls the slowest rate of decay of observed correlation |γ(L)|∼q^{L} and that the complex phase of the eigenvalues determines the oscillations within this exponentially decaying envelope.
We found that the general spectral decomposition of the continuous part of the power spectrum is
Also, recall from earlier that the δ-functions of the power spectrum arise from T's eigenvalues that lie on the unit circle
where ω_{λ} is related to λ by λ=e^{iωλ}. As long as p+q<1, λ = 1 is the only eigenvalue that lies on the unit circle, so that
with
Putting this all together, we find the complete power spectrum for the (2-1)-GP-(2) Process
Thus, the process exhibits δ-functions from the eigenvalue on the unit circle, continuous Lorentzian-like line profiles emanating from finite eigenvalues inside the unit circle which express the process' chaotic nature, and the unique sinusoidal contribution that can only come from zero eigenvalues.
Whenever p<2q−q, the two nontrivial eigenvalues become complex conjugate pairs. This allows us to rewrite the process' power spectrum in a more transparent way
which is clearly symmetric about ω = 0.
The level of completeness achieved is notable: we calculated these properties exactly in closed form for this infinite-Markov order stochastic process over the full range of possible parameter settings. Moreover, once it constructs the process' S-MSP, the Appendix A 3 goes on to produce the same level of analytical completeness but for predictability.
To analyze the process' predictability, we need the S-MSP of any of its generators. Since we started already with the ϵ-machine, we directly determine its S-MSP. This gives the mixed-state transition matrix W=W that, in turn, suffices for calculating both predictability in this section and the synchronization necessary for prediction in the next.
We construct the S-MSP by calculating all mixed states that can be induced by observation from the start distribution π and then calculating the transition probabilities between them. There are eight such mixed-state distributions over S. However, four of them (δ_{A},δ_{B},δ_{C}, and δ_{D}) correspond to completely synchronized peaked distributions. The four other states are new (relative to the recurrent states) transient states and correspond to transient states of recurrent-state uncertainty during synchronization. Calculating, we find the eight unique mixed-state distributions iteratively from
starting with
are
In this, the transient mixed states η^{w} are labeled according to the shortest word w that induces them. These distributions constitute the set of mixed-states R_{π} of the S-MSP. Moreover, each transition probability from mixed state 〈η^{w}| to mixed state 〈η^{wx}| is calculated as 〈η^{w}|T^{(x)}|1〉. Altogether, these calculations yield the S-MSP of the (2−1)-GP-(2) process, reproduced in Fig. 14 from Fig. 5 for convenience.
As a HMM, the ϵ-machine's S-MSP is specified by the 4-tuple: M_{S−MSP}=(R_{π},A,{W^{(x)}}{x∈A},μ_{0}=δ_{π}), where R_{π} is the set of mixed states just quoted, A is the same observable alphabet as before, {W^{(x)}}{x∈A} is the set of symbol-labeled transition matrices among the mixed states, and 〈δ_{π}|=[10000000] is the start distribution over the mixed states.
With this new linear metadynamic in hand, our next step is to calculate the eigenvalues and projection operators of the internal mixed-state-to-state transition dynamic W=∑x∈AW^{(x)}. W can be explicitly represented in the block-matrix form
where
and T is the same as the state-to-state internal transition matrix of the ϵ-machine from earlier.
W's eigenvalues are thus relatively straightforward to calculate since det(W−λI)=det(A−λI)det(T−λI) implies that
The new eigenvalues introduced by the feedback matrix A are Λ_{A}={0,±p} with ν_{0}=2. The new nonzero eigenvalues (±p), found most easily from Part I's [18] cyclic eigenvalue rule, are associated with diagonalizable subspaces. It is important to note that, while T was only nondiagonalizable along a very special submanifold in parameter space, the mixed-state-to-state metadynamic is generically nondiagonalizable over all parameter settings. This nondiagonalizability corresponds to a special kind of symmetry breaking of uncertainty during synchronization.
W's eigenvectors are most easily found through a two-step process. Specifically, |±p_{A}〉 and 〈±p_{A}| (the solutions of A|±p_{A}〉=±p|±p_{A}〉 and 〈±p_{A}|A=±p〈±p_{A}|) are found first, and the result is used to reduce the number of unknowns when solving the full eigenequations (W|±p_{W}〉=±p|±p_{W}〉) for |±p_{W}〉 and 〈±p_{W}|. Similarly, we can recycle the restricted eigenvectors |λ_{T}〉 and 〈λ_{T}| found earlier for the ϵ-machine to reduce the number of unknowns when solving the more general eigenvector problems for |λ_{W}〉 and 〈λ_{W}| in cases where λ∈Λ_{T}. Performing such a calculation, we find
for λ∈Λ_{T}∖{0,1}. Moreover, W's eigenvectors and generalized eigenvector corresponding to eigenvalue 0 are
Above, we used the notation |0k(m)〉 for indexing generalized eigenvectors introduced in Part I [18].
All nondegenerate eigenvalues have projection operators of the form
However, the degenerate and nondiagonalizable subspace associated with the zero eigenvalue has the composite projection operator
The fact that 〈δ_{π}|λ_{W}〉=0 for all λ∈Λ_{T}∖({1}∪Λ_{A}) is an instantiation of a general result that greatly simplifies the calculations relating to predictability and prediction.
The remaining piece for analyzing predictability is the vector of transition-entropies |H(W^{A})〉. A simple calculation, utilizing the fact that
when ∑in_{i}=d, yields
where log is understood to be the base-2 logarithm log_{2}.
Putting this all together, we can now calculate in full detail the myopic entropy rate h_{μ}(L) that results from modeling the infinite-order (2-1)-GP-(2) process as an order-(L−1) Markov process
After L = 2, this teeters along an exponentially decaying envelope as it approaches its asymptotic value of
Simplifying the terms in the myopic entropy rate yields
and
for the two ephemeral contributions.
For L≥3, we find for odd L
and for even L
This highlights the period-2 nature of the asymptotic decay.
The total mutual information between the observable past and observable future, the excess entropy, is
This is the total future information that can possibly be predicted using past observations. The structure of how this information is unraveled over time is revealed in the excess entropy spectrum from −π<ω≤π
From this, we observe that E=limω→0E(ω).
To analyze the information-processing cost of synchronizing to a process, we need its ϵ-machine S-MSP. We already constructed this in Appendix A 3. Hence, we can immediately evaluate the resources necessary for synchronizing to and predicting the (2-1)-GP-(2) Process.
The only novel piece needed for this is the vector of mixed-state entropies |H[η]〉. A simple calculation yields
where log is again understood to be the base-2 logarithm log_{2}.
An observer, tasked with predicting the future as well as possible, must synchronize to the causal state of the dynamic. The remaining causal-state uncertainty H^{+}(L) after an observation interval of L steps is
More explicitly, for L≥2, this becomes
The total synchronization information accumulated is then
Even after synchronization, an observer must update an average of b_{μ} of its bits of information per observation and must keep track of a net C_{μ} bits of information to stay synchronized, where
An interesting feature of prediction is a process' crypticity
This gives, as a function of p and q, the minimal overhead of additional memory about the past—beyond the information that the future can possibly share with it—that must be stored for optimal prediction.
In summary, we now more fully appreciate, via this rather complete analysis, the fundamental limits on predictability of our example stochastic process. It showed many of the qualitative features, both in terms of the calculation and system behavior, that should be expected when analyzing prediction, based on the more general results of the main development. The procedures can be commandeered to apply to any inference algorithm that yields a generative model, whether classical machine learning, Bayesian Structural Inference [9], or any other favorite inference tool. With a generative model in hand, synchronizing to real world data—necessary to make good predictions about the real world's future—follows the S-MSP metadynamics. The consequence for prediction is that typically there will be a finite epoch of symmetry collapse followed by a slower asymptotic synchronization that allows improved prediction, as longer observations induce a refined knowledge of what lies hidden.
FIG. 1. 
Spectra and eigenvalues: (Top) How spectra emanate from eigenvalues: Coronal spectrogram (far right) combines a discrete-time process' eigenvalues Λ_{T} (far left) of the hidden linear dynamic T together with a frequency-dependent function P(ω) (middle) by wrapping the latter around the unit circle. (Bottom) Coronated horizon (far right) combines the frequency-dependent function P(ω) (far left) together with a process' eigenvalues Λ_{A} of the hidden linear dynamic by unwrapping the unit circle.
FIG. 2. 
Process families for exploring the roles of and interplay between Markov-order R, cryptic-order k, the symmetry-collapse index ν_{0}(W) of the zero eigenvalue of the synchronizing dynamic over mixed states, and the cryptic index ν_{0}(ζ) of the zero eigenvalue of the cryptic operator presentation. We always have k≤R and ν_{0}(ζ)≤ν_{0}(W). Whenever Λ_{W}=Λ_{T}∪{0}, R is finite, R=ν_{0}(W) and k=ν_{0}(ζ). Whenever Λ_{ζ}=Λ_{T}∪{0}, k is finite, whether or not R is, and k=ν_{0}(ζ). When k or R is infinite, the cryptic index and symmetry-collapse index reveal more nuanced features of the cryptic and synchronization dynamics. (a) (4-3)-GM Process of the (R−k)-Golden Mean family with 0≤k=ν_{0}(ζ)≤R=ν_{0}(W)<∞, which generates processes with finite but tunable Markov-order R and cryptic-order k. (b) (4-3)-GP-(3) Process of the (ν_{0}(W)-k)-Golden Parity-(P) family with 0≤k=ν_{0}(ζ)≤ν_{0}(W)<R=∞ whenever P > 1, generates processes with infinite Markov-order R, tunable finite cryptic-order k, and tunable finite symmetry-collapse index ν_{0}(W). (c) (4-3)-GPZ-(3-3) Process of the (ν_{0}(W)-ν_{0}(ζ))-Golden Parity-(P−Z) family with 0≤ν_{0}(ζ)≤ν_{0}(W)<k=R=∞ whenever Z > 1. Markov order is infinite whenever either P > 1 or Z>1. Cryptic-order is infinite when Z > 1. This family generates processes with finite but tunable symmetry-collapse index ν_{0}(W) and cryptic index ν_{0}(ζ).
FIG. 3. 
Evolution of eigenvalues Λ_{T} of the recurrent component of the (5–3)-GM Process's ϵ-machine. Displayed within the unit circle of the complex plane, the trajectory of each eigenvalue follows a line that starts thick blue and ends thin red as the transition parameter p evolves from 1 to 0. In addition to the seven eigenvalues that move from the nontrivial eighth roots of unity towards zero along nonlinear trajectories, the eigenvalue at z = 1 does not change with p.
FIG. 4. 
Table of complexity analyses for finite Markov-order processes. Quantitative data used p = 1/2.
FIG. 5. 
Table of complexity analyses for infinite Markov-order processes. Quantitative data used p = 1/2 and q = 1/3.
FIG. 6. 
RRXOR Process ϵ-machine.
FIG. 7. 
Power spectrum P(ω) and POPI spectrum I(ω) of the RRXOR Process: The first is flat and the second identically zero. One might incorrectly conclude that the RRXOR Process is structureless white noise.
FIG. 8. 
Excess entropy spectrum of the RRXOR Process, together with the eigenvalues of the S-MSP transition matrix W. Among the power spectrum, POPI spectrum, and excess entropy spectrum, only the excess entropy spectrum is able to detect the structure in the RRXOR Process since the structure is beyond pairwise. The eigenspectrum of the MSP of the RRXOR ϵ-machine and the excess entropy spectrum both indicate that the RRXOR Process is indeed structured, with both ephemeral symmetry breaking and leaky periodicities in the convergence to optimal predictability.
FIG. 9. 
MSP of the RRXOR Process' ϵ-machine: Grayed out (and dashed) transitions permanently leave the states from which they came. Recognizing the manner by which these transitions partition the mixed-state space allows simplified spectrum calculations. The directed graph structure is inherently nonplanar. The large blue recurrent state should be visualized as being behind the green transient states; it does not contain them.
FIG. 10. 
Ephemeral h_{⊸}(L) and persistent h_{⇝}(L) contributions to the myopic entropy rate h_{μ}(L). The ephemeral contribution lasts only up to L=ν_{0}(W)=2.
FIG. 11. 
Tails of the myopic entropy convergence h_{μ}(L) shown in Fig. 10 decay according to two different leaky period-three envelopes. The latter correspond to the two qualitatively different types of transient synchronization cycles in the MSP of Fig. 9. One of the transient cycles has a relatively fast decay rate of r_{2}=(1/4){1/3}. While the slower decay rate of r_{1}=(1/2){1/3} dominates h_{μ}(L)'s deviation from h_{μ} at large L.
FIG. 12. 
ϵ-Machine of the (2-1)-GP-(2) Process.
FIG. 13. 
Transition-probability space for the (p, q)-parametrized (2-1)-GP-(2) Process.
FIG. 14. 
S-MSP of the (2-1)-GP-(2) Process.
