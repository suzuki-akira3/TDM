Comparing the ISO-recommended and the cumulative data-reduction algorithms in S-on-1 laser damage test by a reverse approach method
We compare the ISO-recommended (the standard) data-reduction algorithm used to determine the surface laser-induced damage threshold of optical materials by the S-on-1 test with two newly suggested algorithms, both named “cumulative” algorithms/methods, a regular one and a limit-case one, intended to perform in some respects better than the standard one. To avoid additional errors due to real experiments, a simulated test is performed, named the reverse approach. This approach simulates the real damage experiments, by generating artificial test-data of damaged and non-damaged sites, based on an assumed, known damage threshold fluence of the target and on a given probability distribution function to induce the damage. In this work, a database of 12 sets of test-data containing both damaged and non-damaged sites was generated by using four different reverse techniques and by assuming three specific damage probability distribution functions. The same value for the threshold fluence was assumed, and a Gaussian fluence distribution on each irradiated site was considered, as usual for the S-on-1 test. Each of the test-data was independently processed by the standard and by the two cumulative data-reduction algorithms, the resulting fitted probability distributions were compared with the initially assumed probability distribution functions, and the quantities used to compare these algorithms were determined. These quantities characterize the accuracy and the precision in determining the damage threshold and the goodness of fit of the damage probability curves. The results indicate that the accuracy in determining the absolute damage threshold is best for the ISO-recommended method, the precision is best for the limit-case of the cumulative method, and the goodness of fit estimator (adjusted R-squared) is almost the same for all three algorithms.
I. INTRODUCTION
Today, after more than 50 years of laser applications, the laser-induced damage (LID) in optical materials and components remains an important factor limiting the performances of many high-power lasers. Therefore, an accurate evaluation of the laser-induced damage threshold (LIDT) of laser optics is important for both the manufacturers and the users of these lasers. Oversimplifying the LID behavior of materials, two operating regimes of repetitive pulsed lasers are of interest: the “nanosecond” range (pulses from few tens of nanoseconds down to few tens of picoseconds) and the femtosecond range (pulses shorter than a picosecond and down to few tens of femtoseconds). The LID physical mechanism is different for these two regimes. In the nanosecond time range, the LIDT is strongly dependent on the presence of defects, material inhomogeneities, and contaminants, and on their statistical distribution over the surface of the sample under test [1,2]. In contrast to that, in the femtosecond time range, the LIDT is almost independent of factors such as the distribution and the concentration of defects, it is sharply defined, and the damage has many similarities to the ablation of the material [3,4]. Consequently, the processing of the LIDT test data in the femtosecond regime is simpler than in the nanosecond regime.
According to the ISO standards 21254-1,2:2011 [5], the most complete standardized procedure to measure the LIDT is the so-called S-on-1 test, which provides a general understanding of the damage behavior for the investigated specimen and allows one to quantitatively estimate the specimen’s lifetime under laser irradiation (the test is described in Part 2 of Ref [5]). Here S represents the maximum allowable number of laser pulses with a certain pulse energy impinging on (interrogating) the same site in order to test its damage behavior. The main result of the S-on-1 test is a family of the so-called characteristic damage curves, depending on a parameter. Each such curve displays the threshold energy density H (fluence, measured in J/cm^{2}) or the threshold power density E (irradiance, measured in W/cm^{2}) versus the minimum number of pulses inducing damage, N, the parameter for each characteristic curve being the probability to damage the sample, p_{ISO}. In many cases, two characteristic damage curves, one corresponding to the damage probability of 0%, which characterizes the absolute damage threshold or the absolute damage onset, and the other corresponding to the damage probability of 50%, respectively, are considered of utmost interest. By extrapolating the measured characteristic damage curve for 0% damage probability to a much larger number of pulses, a lower LIDT fluence value is obtained, estimating the lifetime of the tested specimen when using that value. Notice that the characteristic damage curve corresponding to the 0% damage probability is also named in some papers the “laser survivability curve.” [6] Currently, the S-on-1 test is used in many laboratories to evaluate the laser-damage performance of laser optics [7–9].
Without going into the details of the standardized S-on-1 test, assumed familiar to the reader, the key element of the standard data-reduction algorithm is the calculation of the damage probability p_{iISO} corresponding to approximately constant pulse energy Q_{i}, which interrogates several sites of the specimen. This energy level lies within a constant energy interval of width ΔQ, the same for all the orders i of the energy levels; i = 1, 2, …, I; Q_{1} < Q_{2} < … < Q_{I}, where I is the total number of energy levels. As a result of the interrogation at the energy level Q_{i}, a certain number of sites will be damaged, denoted by n^{D}_{i}, and another number of sites, denoted n^{ND}_{i}, will be non-damaged. This damage probability, p_{iISO}, is calculated from the number of damaged and non-damaged sites as:
The range of the pulse energies Q_{i} employed in the test must be sufficiently broad to cover sites with 0% damage probability as well as sites with 100% damage probability.
The damage probability p_{iISO} depends on the laser pulse energy Q_{i} and on the minimum number of pulses inducing the damage, N_{min}, where N_{min} ≤ S, i.e., p_{iISO} = p_{iISO}(Q_{i}, N_{min}). Once the damage occurs for N_{min}, the arrival of the subsequent laser pulses on the damaged site is automatically blocked and another site begins to be interrogated. Usually, the probability is calculated for a fixed number of pulses N, selected to form a sequence of 8-10 numbers written “almost equally spaced” according to a logarithmic scale, as, for example, N ∈ {1, 2, 5, 10, 20, 50, 100, 200, 500} [9]. Assuming a site irradiated at the energy Q_{i} is damaged for a number of pulses lying in-between two numbers of this sequence, for example, N_{min} = 67 pulses, it is considered a damaged site for all N > N_{min} (i.e., for N = 100, 200, 500 of the above sequence) and a non-damaged one for all N < N_{min} (i.e., for N = 1, 2, 5, 10, 20, 50 of the above sequence). Therefore, we can write
To rescale the pulse energy arriving on target Q into fluence H and irradiance E, the quantities named effective area of the spot on target, A_{eff}, and the effective pulse duration, t_{eff}, respectively, defined in Part 1 of Ref [5], are used. These are measurable quantities [10,11]. By plotting H(N) or E(N) with p_{ISO} as parameters, all approximated now as continuous functions and a continuous parameter, respectively (not depending on the subscript i), the family of characteristic damage curves is obtained.
The total number of irradiated sites of the sample, n_{t}, is given by summing over all energy intervals the number of interrogated sites, damaged and non-damaged, corresponding to each order i of the energy interval, i.e., n^{D}_{i} + n^{ND}_{i}. Hence,
This number is limited by the sample size and the spot size on the sample, resulting n_{t} ∼ 100–300 for a sample of 25 mm diameter. Therefore, the statistical uncertainty, or the error bar σ_{i} = 1/(n^{D}_{i} + n^{ND}_{i}) for each damage probability point p_{iISO}(Q_{i}, N), is limited by the number of available damaged and non-damaged sites within the specific energy interval of order i. Also, the fitting error for the regression of the damage probability points (by using a certain fitting curve) is limited by the number of probability points calculated for each N. The number of probability points equals the total number of energy intervals, I.
To improve the statistical uncertainty σ_{i} mentioned above, a new data-reduction algorithm to calculate the damage probability points was recently suggested, the originally called “cumulative damage probability approach,” [12] and later called the “damage probability calculation by redistributing data treatment,” [13] here named the “regular cumulative method” (Cu). According to this algorithm, within the energy interval corresponding to the energy level Q_{i}, one takes into account not only the damaged and the non-damaged sites belonging to that interval [as in the standard, conventional ISO method, Eq. (1)], but also the damaged sites belonging to intervals corresponding to all energy values lower than or equal to Q_{i}, and the non-damaged sites belonging to intervals corresponding to energy levels equal to or higher than Q_{i}. This approach increases (though artificially, but justifiably) the number of counted test sites per energy interval of interest (i.e., corresponding to Q_{i}), for the same total number of available test sites, and so, it improves (decreases) the uncertainty σ_{i} of each damage probability point, as defined above. Note that within this approach, the probability to have a damaged site by irradiating it with the energy Q_{i}, here denoted p_{iCu}, is, therefore, defined as [12,13]
where j is the current summation index corresponding to the pulse energy level Q_{j}, Q_{1} ≤ Q_{j} ≤ Q_{I}, I is the total number of energy intervals, as defined earlier, and n^{D}_{j} and n^{ND}_{j} represent the number of damaged and non-damaged sites within the energy level Q_{j}, respectively. Notice the different lower and upper limits of the sums in Eq. (4), to account for the “cumulative,” or the “redistributed” number of damaged and non-damaged sites corresponding to the energy interval Q_{i}.
Similar to the standard case, Eq. (2), the damage probability calculated by the regular cumulative method, p_{iCu}, is still a function of the pulse energy level Q_{i} and the selected number of pulses inducing the damage, N, i.e.,
Notice that the name “cumulative damage probability approach” used in Ref [12], and renamed “damage probability calculation by redistributing data treatment” in Ref [13], central to this work, refers only to the way of processing the data of a LID experiment (real or simulated). It is completely different than the mathematical concept of “cumulative distribution function” [14] or than some physical models allowing to calculate the probability that a site is damaged for a certain fluence, a certain distribution of defects, and a certain beam profile—the latter probability named also “cumulative probability of damage” [15]—all of these names leading to an unpleasant source of confusing terminology.
A further extension of the regular cumulative method (Cu) was made by searching its limit case (here denoted Cu-L), when instead of using the total number I of energy intervals ΔQ, corresponding to pulse energy levels Q_{1}, Q_{2}, …, Q_{I}, the total number of irradiated sites, n_{t} ≫ I, is used to calculate the damage probability for each site and also for each number of laser pulses inducing the damage from the selected sequence, N [16]. In this limit case, the sites are numbered in increasing order of their irradiated energy and, correspondingly, the damage probability, denoted here p_{kCu-L}, is calculated for each site. This approach leads to a much higher number of probability points, and therefore to a better fit of the damage probability curve for each number of pulses N, as compared to the ISO or the regular cumulative method. In this limit case, the damage probability p_{kCu-L} is defined as
In Eq. (6), j is the current summation index representing the jth site subjected to the energy level Q_{j}, k is the index representing the site of interest, in which the damage probability is calculated, and n^{D}_{j} and n^{ND}_{j} represent the number of damaged and non-damaged sites within the energy level Q_{j}, respectively. Notice that, in contrast to Eq. (4), the upper summation index of the last sum is n_{t}, the total number of irradiated sites on the sample.
Similar to Eq. (5), we can say that the limit-case damage probability for the site k depends on the pulse energy irradiating that site and the number of damaging laser pulses N, i.e.,
In spite of these improvements, the principle drawback of all data-reduction algorithms, i.e., the lack of an analytical proof of their validity, continues to remain and requires new studies to check the appropriateness of these algorithms.
To compare the standard (ISO) to the regular cumulative data-reduction method, a reverse approach was recently suggested [17] and also used in the subsequent work [13]. This approach consists in generating test-data of damaged and non-damaged (survived) test sites based on given, known damage probability distribution functions and damage threshold values, and then assessing the effectiveness of the different algorithms by comparing the precision and accuracy of their results to the original assumed functions and values. In this way, additional errors due to a real experiment, as fluctuations of laser parameters, sample non-uniformity, errors of the measuring instruments, are avoided. Unfortunately, no information is presented in Ref [17] concerning the technique used to generate a test-data from an assumed damage probability distribution function. Consequently, these results have to be confirmed by new and explicit reverse-approach simulated experiments.
This paper reports a comparison of data-reduction algorithms used in laser-induced damage tests, i.e., the ISO-recommended approach (ISO), described in Part 2 of Ref [5], the regular cumulative approach (Cu) [12,13], and its limit case (Cu-L) [16], by explicitly using the reverse approach. Section II describes three assumed damage probability distribution functions used to generate the test-data in our investigation. Section III describes four reverse approach techniques which we used to generate a number of twelve sets of test-data, based on the assumed probability distribution functions specified in Sec. II. In Sec. IV, the quantities used to compare the algorithms mentioned above characterizing the accuracy and the precision in establishing the absolute damage threshold (damage onset) are defined, and the results are presented and discussed, including the goodness of the fit estimator. Section V concludes the paper.
II. ASSUMED PROBABILITY DISTRIBUTION FUNCTIONS USED TO GENERATE THE TEST-DATA BY THE REVERSE APPROACH METHOD
In this investigation, we use three different assumed probability distribution functions, two of them supposedly specific for defect-driven damage models [18,19], and the third one a linear distribution function which many times is used in practice according to the ISO 21254-2 standard [5]. The first two types of these functions were analyzed and derived for a target subjected to a Gaussian laser spot, specific for the LIDT with nanosecond pulses.
For all the three analyzed probability distribution functions, we considered a rotationally symmetric focused Gaussian laser spot in the target plane, as it is usual in the S-on-1 tests [5]. Its fluence spatial profile, H(r), is given by
where H_{a} is the axial or the peak fluence, r is the radius of a circle centered on the beam axis with an area A = πr^{2}, and w is the spot radius defined such that H(w) = (1/e^{2})H_{a}. Notice that this is the definition used in the ISO 11146-1 Standard [20], with the result that πw^{2}/2 = A_{eff} is the effective area of the focused Gaussian spot. Equation (8) can be rewritten as
By denoting H_{T} as the damage threshold fluence (H_{T} ≤ H_{a}) and by using H(r) ≥ H_{T} in Eqs. (8) or (9), we get the area Α of the inner circle of the laser spot in which the current fluence equals or exceeds the damage threshold fluence value
This area can be named the threshold irradiation area, the analog of the quantity named clip-level irradiation area of the ISO 13694 Standard [21].
A. Two probability distribution functions based on defect-driven damage models
In this subsection, we consider two probability distribution functions specific for defect-driven LIDT, encountered in nanosecond regime LID. They were discussed at large in Refs [18] and  [19].
The first probability density function is considered for a degenerate defect ensemble, i.e., when all the defects have the same damage threshold, and for a Gaussian spatial profile of laser irradiation. In this case, the surface damage probability distribution p_{D-deg} is written as [see Eq. (15) of Ref [18], in our notations]
Here H_{M} is the maximum fluence value used in the LID experiments, and the normalizing constant D represents the surface (area) density of the defects in units of number of defects/area. Note that p_{D-deg} = 0 for H_{a} = H_{T}. Given the quantities H_{T} and A_{eff}, the defect density D can be determined from Eq. (11) by setting the value of H_{a} such that H_{a} = H_{M} and the corresponding value of the p_{D-deg}(H_{M}/H_{T}, D). Equation (11) represents a two-parameter model of the damage probability distribution, the two parameters being H_{T} and D. Note that p_{D-deg} of Eq. (11) is always less than 1, but it can closely approach this value for an appropriately selected total number of defects within the laser spot A_{eff}D, and a ratio H_{M}/H_{T} > 1. For example, for A_{eff}D = 10 and H_{M}/H_{T} = 1.6, one obtains p_{D-deg} ≈ 0.991.
The second probability distribution function corresponds to the case of surface damage for a nondegenerate defect ensemble, i.e., for a collection of nonidentical isolated defects distributed on the sample surface, each of them characterized by the local peak fluence H_{a} required to produce damage. In this case, the defect population can be specified by a defect ensemble function D_{d}(H_{a}), with H_{a} ≥ H_{T}. This function gives the number of defects per unit area that induces damage at a mean local fluence between H_{a} and H_{a} + dH_{a}. This represents a three-parameter model of the sample’s surface that provides a curvature flexibility for the function D_{d}(H_{a}) via an additional integer parameter p, where p > −1. This model is discussed in Refs [18] and  [19]. Note that we keep the original notation for the parameter p, which is not a probability, but just an integer exponent. Accordingly, for p = 0, the case we consider in this work, the function D_{d}(H_{a}) is constant for H_{a} ≥ H_{T}, which means that at each fluence level at or above the threshold H_{T}, there are an equal number of defects whose damage threshold is at the new value of H_{a} (see Figs. 4 and 5 of Ref [18] and Sec. III of Ref [19]). From Ref [19] it follows that on a test surface with such type of defect ensemble, each new fluence interval of equal width ΔH_{a}, from H_{T} to H_{M}, adds also a constant number Δn^{D} of damaged sites. Here ΔH_{a} = (H_{M} − H_{T})/k, where k is the number of equal fluence intervals dividing the working fluence range. In this case, the number of damaged sites for each fluence interval, n^{D}_{i}, i ∈ {1, …, k}, is given by
Specific closed-form solutions of the three-parameter damage probability model for different values of p ∈ [0, 1] are presented in Ref [19]. The surface damage probability distribution for the case p = 0 is [see Eq. (33) and Table 1 of Ref [19]]
valid for H_{a} ∈ [H_{T}, H_{M}]. To specify the function p_{D}(H_{a}), the quantities H_{T}, A_{eff}, C_{0}, and H_{M} have to be set or derived. As shown in Sec. III, the property described by Eq. (12) and the damage probability distribution of Eq. (13) are used to develop two data sets.
B. Linear damage probability distribution function and possibly S-shaped nonlinear distribution functions
Here we consider the third distribution function, a linear one, as recommended by the current ISO 21254-2 standard [5]. In principle, it can be applied to calculate the LIDT with nanosecond, as well as with femtosecond laser pulses.
This function describes the sample’s surface damage probability that linearly depends on the axial peak fluence H_{a} of the applied Gaussian fluence laser spot of Eq. (8), p_{D-L}(H_{a}), and it is given by the following equation:
For H_{a} ∈ [H_{T}, H_{M}], the range of p_{D-L}(H_{a}) function is [0, 1]. This probability function is specified by setting the quantities H_{T} and H_{M}.
Notice that it is possible to consider also various S-shaped nonlinear distribution functions to represent more realistic probability distributions, as the sigmoid function, the logistic function, the generalized logistic functions, and the Gompertz function [22]. However, the fit using these nonlinear functions is beyond our present paper and might be considered for future work.
An interesting approach, by considering a more realistic S-shaped probability distribution function due to the fluctuations of the laser fluence and a maximum likelihood estimation fitting algorithm (see also Sec. 6 of Ref [2]), and which considers both, simulated (virtual) and real experiments, is presented in Ref [23].
III. REVERSE TECHNIQUES TO GENERATE TEST-DATA USING KNOWN PROBABILITY DISTRIBUTION FUNCTIONS
Here we describe four reverse techniques (denoted RT1 to RT4) which we used to generate a number of 12 sets of test-data based on the assumed damage probability distribution functions given by Eqs. (11), (13), and (14). The ranges of the three probability distribution functions analyzed here, corresponding to the fluence interval [H_{T}, H_{M}], lie from [0, 0.95] to [0, 0.99]. Irrespective of the reverse techniques used, the same value of the damage threshold fluence, H_{T} = 10 J/cm^{2}, was assigned to each set of test-data. The maximum fluence, corresponding to the maximum value of the damage probability, H_{M}, equals 3H_{T} for most of the test data and 2H_{T} for test-data #3 and #10 (see also the abscissa of the diagrams of Fig. 1). Some other parameters, specific to one or the other of the reverse techniques, are presented in the last column of Table I and also in the description of each reverse technique, below.
RT1 technique is based on the distribution function of Eq. (11), corresponding to a degenerate defect ensemble. To specify the function p_{D-deg} and the laser spot size, we set for this case the quantities A_{eff} = 5 × 10^{−4} cm^{2}; p_{D-deg}(H_{M},D) = 0.99. To generate test-data using Eq. (11), a Monte Carlo based LIDT measurement model has been created, as it was proposed in Refs [23] and  [24]. The model imitates a plane sample surface covered with randomly distributed defects of D total density and with the same damage threshold H_{T}. The sample surface is divided into vertical sections corresponding to 30 fluence levels. The horizontal sections define the number of sites tested at a particular fluence. Thus the subdivided surface could be interpreted as a matrix of test sites used in real LIDT measurements. Each of the subdivided section is interrogated by irradiating it with a centered circular spot of specific area A(H_{a},H_{T}) given by Eq. (2). Damage is considered when the applied localized fluence is higher than the defined threshold of the irradiated defect. For each fluence level, a number of 13 subdivided sections were interrogated.
RT2 technique uses a different LIDT measurement model based on the same distribution function given by Eq. (11): the whole sample surface of randomly distributed defects of D total defect density and H_{T} damage threshold is subdivided into a matrix of 400 test sites, and the fluence range [H_{T}, H_{M}] is also divided into 400 fluence levels. Then, each test site is interrogated at random with one of the established ones of the 400 fluence levels, by overlapping a centered circular spot of a specific area given by Eq. (10). Similarly, damage is considered when the applied localized fluence is higher than the threshold H_{T} of the corresponding irradiated defect.
RT3 technique is based on the distribution function of Eq. (13), considered for p = 0 (i.e., when the function D_{d}(H_{a}) is constant for H_{a} ≥ H_{T}). It also relies on the property given by Eq. (12) of a data-base which is generated by using the distribution function of Eq. (13). For example, the fluence range [H_{T}, H_{M}] is divided into a series of 20 intervals of equal widths, denoted from I_{1} to I_{20}, starting with the threshold fluence. In the first interval, I_{1}, one considers one damaged site n^{D}_{1} = 1, and in the following intervals, the number of damaged sites n^{D}_{i}, i ∈ {2, 3, …, 20} is determined according to Eq. (12), for Δn_{D} = 1. This means a resulted total number of damaged sites n^{D}_{T} = 210 for this set of test-data. The corresponding number of non-damaged sites n^{ND}_{i} for each interval I_{i} is derived from the relation
where H_{ai} is the mean fluence value for each interval I_{i} and p_{D}(H_{ai}) is given by Eq. (13) when the parameters H_{T}, H_{M}, A_{eff}, and C_{0} are set or derived.
RT4 technique was developed based on two general assumptions, denoted A1 and A2 as follows:
A1. The fluence separation between the damaged sites increases according to the survival probability (1 − p_{D}(H_{a})), when the fluence H_{a} decreases from the H_{M} to H_{T} values. So, the sequence of the decreasing fluences of the damaged sites (with subscript D) starts from H_{a1-D} = H_{M} and can be written as follows:
where the quantity ΔH (in J/cm^{2}) is the separation parameter and δH ≤ ΔH defines a fluence level H_{T} + δH near the threshold, where p_{D}(H_{T} + δH) is nonzero.
A2. The fluence separation between the survived sites increases according to damage probability p_{D}(H_{a}), when the fluence H_{a} increases from the H_{T} + δH value up to the value H_{M}. So, the sequence of the increasing fluences of the non-damaged (subscript ND) sites starting from H_{a1-ND} = H_{T} + δH is as follows:
The desired total number of sites within a set of the test-data is obtained by setting the parameter ΔH. Unlike the RT1–RT3 techniques, which can be applied only on specific defect damage models, the RT4 recurrence technique is a general one, which can be used to generate sets of test-data based on any arbitrary damage probability distribution. Consequently, we have used the technique RT4 to generate different sets of test-data from Eqs. (13) and (14).
IV. QUANTITIES OF INTEREST, RESULTS, AND DISCUSSION
A. Quantities of interest
Based on Eqs. (11)–(15), a number of 12 sets of test-data were generated by using the MATLAB 8.4 software [25] and the four reverse techniques described above. Each of the generated set of test-data was processed in three different ways using the ISO, the Cu, and the Cu-L data-reduction algorithms, respectively. The resulted three sets of discrete damage probability points versus fluence were fitted with the corresponding original, assumed damage probability distribution function, by using the Levenberg-Marquardt algorithm [26] to fit the nonlinear probability functions and the regular least-square method (for the linear fit), implemented in the commercial software Origin 8.1 [27]. From each fitted curve, a value of the fitted damage threshold, H_{T-fit}, was determined, and the following two main quantities were calculated and used to compare the three data reduction algorithms: the relative error in establishing the absolute damage threshold (onset) ε_{th} and the relative standard error ε_{H} associated with H_{T-fit}. They are defined as follows:
• The relative error ε_{th} in estimating the absolute damage threshold H_{T} is defined as
where H_{T-fit} is the estimation of the absolute damage onset given by the fitted curves of damage probability. This relative error characterizes the accuracy in estimation of the absolute damage onset, i.e., how close to the 0% damage threshold H_{T} of the original function is the estimated H_{T-fit} value.
• The relative standard error ε_{H} associated with H_{T-fit} is
where s_{H} is the standard error (i.e., the standard deviation) of the H_{T-fit}, directly obtained from the commercial software Origin 8.1 [27] used to fit the damage probability curves.The relative standard error ε_{H} characterizes the precision in the determination of the H_{T-fit} quantity.
The goodness of fit was estimated using the adjusted R-squared fit estimator (Adj. R-square in the diagrams of Fig. 1), calculated, and displayed by the Origin 8.1 software algorithm [28] although there are contradicting opinions of using this estimator for nonlinear fitting within the software community [29].
B. Results
Table I summarizes the main numerical results obtained by this study, displaying the values of H_{T-fit}, ε_{th}, ε_{H}, and other parameters used in our simulations, for all 12 test-data used and for the three data-reduction algorithms considered: ISO, Cu, and Cu-L. Note the same assigned value of H_{T} for all data sets, H_{T} = 10 J/cm^{2}.
Figure 1 displays the original, assumed probability functions used in the simulations and the probability points recovered using the three different tested algorithms (ISO, Cu, and Cu-L), for each of the data-set, #1 to #12. For clarity reasons, the fitted damage probability curves are not plotted in these diagrams.
C. Discussion
From Table I, one can see that the ISO was the most accurate method in determining the absolute damage threshold (the onset value), i.e., the mean value of the ε_{th} error is 1.1%, over four times smaller in comparison to the cumulative methods, and its relative error for the fitted threshold, ε_{H}, is only 1.0%. This can be seen also in Fig. 1, where it is obvious that the sets of probability points given by the ISO method better follow the original distribution function than the points of the cumulative methods. As expected, the Cu-L method was the most precise one, i.e., it has a significantly lower mean value of the ε_{H} error in comparison to the ISO and the Cu cases, due to an important increase of the number of the damage probability points in this case. The adjusted R-squared fit estimator for all 12 data-sets can be read from the diagrams of Fig. 1 and has values between 0.92 and 0.99.
An interesting finding is that the cumulative methods provide inaccurate results when these methods are used to calculate the damage probability points considering only a part of the available data from a given set of test-data (e.g., only the test sites included in the first half of the fluence interval, [H_{T}, (H_{T} + H_{M})/2]. In this case, because the probability points do not take into account the survived sites existing in the upper half interval [(H_{T} + H_{M})/2, H_{M}], they get higher values, and they approach too early the maximum value corresponding to the original probability distribution function. In contrast to this behavior, the ISO-recommended method maintains the calculated probability points close to the original assumed distribution function. Figure 2 shows this behavior of the three methods (ISO, Cu, and Cu-L) at data-processing using only the first half of the available fluence range, for the data sets #10 and #12, respectively. This ISO method behavior is important in real damage testing, when the sample response in the fluence range around its absolute damage onset (the 0% damage threshold level) is investigated.
In our opinion, the most important result of this reverse approach analysis is that the ISO-recommended method clearly is the most accurate in the evaluation of the absolute damage onset of the original distribution functions, as compared to the two cumulative methods (the regular and its limit-case). This is somewhat a surprising result because it is obvious that the Cu and Cu-L methods improve the distribution of damage probability points versus fluence or increase the number of resulted damage probability points [12,13,16,17] in comparison to the ISO one. Also, our result does not confirm the data published in Ref [17] where the associated errors and the goodness of fit of the damage probability curves resulted from the reverse evaluation are significantly better when applying the regular cumulative method versus the ISO one. We believe that more comparative analysis of test-data generated by explicit reverse techniques is necessary to be performed, to clarify whether or not any of the cumulative (Cu or Cu-L) data-reduction could really improve the accuracy in evaluating the 0% damage threshold of optical materials, in comparison to the ISO-recommended technique.
V. CONCLUSION
A number of 12 sets of test-data of survived and damaged test sites were generated by using four different reverse techniques and then were investigated by applying the ISO-recommended and two so-called cumulative (the regular one and its limit case) data-reduction methods. As assumed damage probability distribution functions, we have used two functions based on damage-defect models and one linear function, the latter being recommended by the current ISO 21254-2 standard. The main findings of this study are as follows:
• The ISO method provides the most accurate data-reduction algorithm in evaluating the absolute damage onset of the assumed damage probability functions, the average ε_{th} error on all the 12 processed sets being about 1.1%, more than four times smaller in comparison to the cumulative methods.
• The limit case of the cumulative method (Cu-L) assures the best precision, the average ε_{H} of 0.23% being about four times smaller than for the other two methods.
• The regular and the limit cumulative methods could also provide reasonable low level of the relative errors associated with the fit parameter characterizing the damage onset, H_{T-fit} (a mean relative error ε_{th} of 4.75% and 4.22%, respectively), when they are applied on the entire test-data.
• All three methods provide a good quality of fit (adjusted R-squared fit estimator between 0.92 and 0.99), within the limits of the appropriateness of using this quantity [28,29].
• The use of the full range of the available fluence, from the threshold (H_{T}) to the maximum value corresponding to damage (H_{M}) is important especially when testing the two cumulative data reduction algorithms in LID studies.
TABLE I. 
Summary of the main results and of some specific parameters used in the study.
TABLE I. -body
Set#	Eq#_RT#	ISO	Cu	Cu-L	Parameters D (defects/cm^{2})
											ΔH (J/cm^{2})
		H_{T-fit} (J/cm^{2})	ε_{th} (%)	ε_{H} (%)	H_{T-fit} (J/cm^{2})	ε_{th} (%)	ε_{H} (%)	H_{T-fit} (J/cm^{2})	ε_{th} (%)	ε_{H} (%)	p_{D}(H_{M}) ∈ [0.95, 0.99]
1	Eq11_RT1	10.13	1.34	2.16	10.03	0.32	3.09	10.28	2.81	0.31	D = 8 400
2	Eq11_RT1	10.26	2.60	1.50	10.26	2.55	1.70	10.21	2.09	0.16	D = 5 500
3	Eq11_RT2	10.22	2.18	0.66	10.20	1.96	0.65	10.11	1.06	0.07	D = 8 400
4	Eq11_RT2	10.10	0.98	1.22	10.09	0.90	1.89	10.06	0.57	0.16	D = 5 500
5	Eq13_RT3	10.05	0.55	0.57	10.21	2.13	2.32	10.22	2.22	0.42	D = 6 600
6	Eq13_RT3	9.92	0.79	1.11	10.17	1.72	1.30	10.28	2.76	0.35	D = 13 300
7	Eq13_RT4	10.04	0.37	0.66	10.93	9.32	0.75	10.88	8.83	0.18	D = 10 200; ΔH = 1
8	Eq13_RT4	10.06	0.63	1.02	10.95	9.54	0.83	11.13	11.31	0.25	D = 10 200; ΔH = 0.64
9	Eq13_RT4	10.00	0.03	1.03	10.19	1.91	2.34	10.22	2.19	0.42	D = 6 600; ΔH = 1
10	Eq13_RT4	9.99	0.07	0.63	10.47	4.67	0.40	10.45	4.52	0.13	D = 30 000; ΔH = 0.37
11	Eq13_RT4	10.06	0.64	1.32	10.86	8.59	1.03	10.39	3.85	0.23	D = 8 700; ΔH = 1
12	Eq14_RT4	10.29	2.90	0.15	11.34	13.38	0.33	10.84	8.38	0.05	ΔH = 0.5
Average values	10.09	1.09	1.00	10.47	4.75	1.39	10.42	4.22	0.23	
FIG. 1. 
Diagrams of three sets of discrete probability points plotted together with the original distribution function, as resulted from ISO, Cu, and Cu-L calculations, for all sets (#1 to #12) of test-data generated by the reverse approach.
FIG. 2. 
The original distribution function and the calculated damage probability points using only the first half of the fluence range, [H_{T}, (H_{T} + H_{M})/2], for the ISO, Cu, and Cu-L methods. (a) For data set #10. (b) For data set #12.
